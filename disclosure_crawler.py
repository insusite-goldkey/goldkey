# ==========================================================================
# [ë³´í—˜ ê³µì‹œì‹¤ ì‹¤ì‹œê°„ ì•½ê´€ í¬ë¡¤ëŸ¬] disclosure_crawler.py
#
# ì—­í• : ë³´í—˜ì‚¬ ìƒí’ˆê³µì‹œì‹¤ì—ì„œ íŠ¹ì • ìƒí’ˆÂ·ê°€ì…ì¼ìì— ë§ëŠ” ì•½ê´€ PDFë¥¼
#       ì‹¤ì‹œê°„ íƒìƒ‰Â·ë‹¤ìš´ë¡œë“œÂ·RAG ì¸ë±ì‹±í•˜ëŠ” JIT(Just-in-Time) íŒŒì´í”„ë¼ì¸.
#
# ì„¤ì¹˜:
#   pip install playwright pdfplumber requests
#   playwright install chromium
# ==========================================================================

import io, re, time, hashlib, requests, logging
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from datetime import date, datetime
from typing import Optional, List, Tuple

logger = logging.getLogger("disclosure_crawler")


# ---------------------------------------------------------------------------
# 0. AI í¬ë¡¤ë§ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ---------------------------------------------------------------------------
CRAWLER_SYSTEM_PROMPT = """[ì—­í• ] ë„ˆëŠ” ë³´í—˜ ì•½ê´€ ì „ë¬¸ í¬ë¡¤ë§ ë° ì¸ë±ì‹± ì—ì´ì „íŠ¸ì•¼.

[ì„ë¬´]
1. ì‚¬ìš©ìê°€ ì…ë ¥í•œ [ë³´í—˜ì‚¬ëª…, ìƒí’ˆëª…, ê°€ì…ì¼ì]ë¥¼ í™•ì¸í•´.
2. í•´ë‹¹ ë³´í—˜ì‚¬ì˜ 'ìƒí’ˆê³µì‹œì‹¤' URL ê·œì¹™ì„ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´.
3. íŒë§¤ ê¸°ê°„(íŒë§¤ê°œì‹œì¼ ~ íŒë§¤ì¢…ë£Œì¼)ì´ ê°€ì…ì¼ìë¥¼ í¬í•¨í•˜ëŠ” PDFë§Œ ì„ íƒí•´.
4. ì—¬ëŸ¬ ë²„ì „: (a) íŒë§¤ ê¸°ê°„ í¬í•¨ ìš°ì„  (b) ë™ì¼ ì¡°ê±´ì´ë©´ ê°œì •ì¼ ìµœì‹  ì„ íƒ.
5. ë‹¤ìš´ë¡œë“œëœ PDFë¥¼ JIT RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¦‰ì‹œ ì „ë‹¬í•´.

[ì¶œë ¥] ì„ íƒ PDF URL / íŒë§¤ ê¸°ê°„ / ê°œì •ì¼ / ì‹ ë¢°ë„(0~100%) / ì„ íƒ ê·¼ê±°

[ê¸ˆì§€] ê°€ì…ì¼ ë²”ìœ„ ì™¸ ì•½ê´€ / ë¹„ê³µì‹ ê²½ë¡œ(ë¸”ë¡œê·¸Â·2ì°¨ì‚¬ì´íŠ¸) / ì•½ê´€ ì„ì˜ ìˆ˜ì •"""


# ---------------------------------------------------------------------------
# 1. ë³´í—˜ì‚¬ë³„ ê³µì‹œì‹¤ URL ë ˆì§€ìŠ¤íŠ¸ë¦¬
# ---------------------------------------------------------------------------
class CompanyUrlRegistry:
    _REG: dict = {
        "ì‚¼ì„±ìƒëª…":    {"base": "https://www.samsunglife.com",    "url": "https://www.samsunglife.com/customer/publicInfo/productDisclosure.do",    "p": "searchKeyword"},
        "í•œí™”ìƒëª…":    {"base": "https://www.hanwhalife.com",      "url": "https://www.hanwhalife.com/cust/disclosure/productlist.do",               "p": "searchWord"},
        "êµë³´ìƒëª…":    {"base": "https://www.kyobo.co.kr",         "url": "https://www.kyobo.co.kr/prd/disclosures/productDisclosure",              "p": "keyword"},
        "ì‹ í•œë¼ì´í”„":  {"base": "https://www.shinhanlife.co.kr",   "url": "https://www.shinhanlife.co.kr/hp/cdha0100.do",                           "p": "searchWord"},
        "NHë†í˜‘ìƒëª…":  {"base": "https://www.nhlife.co.kr",        "url": "https://www.nhlife.co.kr/disclosure/product",                            "p": "searchText"},
        "ë¯¸ë˜ì—ì…‹ìƒëª…":{"base": "https://life.miraeasset.com",     "url": "https://life.miraeasset.com/csc/disclosure/productTerms.do",             "p": "searchWord"},
        "DBìƒëª…":      {"base": "https://www.db-life.com",         "url": "https://www.db-life.com/customer/publicInfo/product.do",                 "p": "keyword"},
        "ì‚¼ì„±í™”ì¬":    {"base": "https://www.samsungfire.com",     "url": "https://www.samsungfire.com/cust/disclosure/productDisclosure.do",        "p": "searchKeyword"},
        "í˜„ëŒ€í•´ìƒ":    {"base": "https://www.hi.co.kr",            "url": "https://www.hi.co.kr/cms/disclosure/product/list.do",                    "p": "searchKeyword"},
        "DBì†í•´ë³´í—˜":  {"base": "https://www.idb.co.kr",           "url": "https://www.idb.co.kr/cust/disclosure/product.do",                       "p": "keyword"},
        "KBì†í•´ë³´í—˜":  {"base": "https://www.kbinsure.co.kr",      "url": "https://www.kbinsure.co.kr/cust/disclosure/product.do",                  "p": "searchWord"},
        "ë©”ë¦¬ì¸ í™”ì¬":  {"base": "https://www.meritzfire.com",      "url": "https://www.meritzfire.com/disclosure/product-announcement/product-list.do", "p": "searchKeyword"},
        "ë¡¯ë°ì†í•´ë³´í—˜":{"base": "https://www.lotteins.co.kr",      "url": "https://www.lotteins.co.kr/cust/disclosure/product.do",                  "p": "keyword"},
        "í•œí™”ì†í•´ë³´í—˜":{"base": "https://www.hwgeneralins.com",    "url": "https://www.hwgeneralins.com/cust/disclosure/product.do",                "p": "keyword"},
        "í¥êµ­í™”ì¬":    {"base": "https://www.heungkukfire.co.kr",  "url": "https://www.heungkukfire.co.kr/cust/disclosure/product.do",              "p": "keyword"},
        "ìƒëª…ë³´í—˜í˜‘íšŒ":{"base": "https://klia.or.kr",              "url": "https://klia.or.kr/consumer/publicRelation/productDisclosure.do",        "p": "searchKeyword"},
        "ì†í•´ë³´í—˜í˜‘íšŒ":{"base": "https://www.knia.or.kr",          "url": "https://www.knia.or.kr/consumer/publicRelation/productDisclosure.do",    "p": "searchKeyword"},
    }
    _ALIAS: dict = {
        "ì‚¼ì„±": "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±í™”ì¬ë³´í—˜": "ì‚¼ì„±í™”ì¬",
        "í˜„ëŒ€": "í˜„ëŒ€í•´ìƒ", "í˜„ëŒ€í•´ìƒí™”ì¬": "í˜„ëŒ€í•´ìƒ",
        "DB": "DBì†í•´ë³´í—˜", "ë™ë¶€í™”ì¬": "DBì†í•´ë³´í—˜", "ë™ë¶€ìƒëª…": "DBìƒëª…",
        "KB": "KBì†í•´ë³´í—˜", "í•œí™”": "í•œí™”ìƒëª…",
        "ë†í˜‘": "NHë†í˜‘ìƒëª…", "ë¯¸ë˜ì—ì…‹": "ë¯¸ë˜ì—ì…‹ìƒëª…", "ë©”ë¦¬ì¸ ": "ë©”ë¦¬ì¸ í™”ì¬",
        "ë¡¯ë°": "ë¡¯ë°ì†í•´ë³´í—˜", "í¥êµ­": "í¥êµ­í™”ì¬",
        "ì‹ í•œ": "ì‹ í•œë¼ì´í”„", "ì˜¤ë Œì§€ë¼ì´í”„": "ì‹ í•œë¼ì´í”„",
        "AIA": "AIAìƒëª…", "ì²˜ë¸Œ": "ì²˜ë¸Œë¼ì´í”„",
    }
    # ì†í•´ë³´í—˜ì‚¬ í‚¤ì›Œë“œ (ë¯¸ë“±ë¡ ì‹œ ì†ë³´í˜‘íšŒë¡œ í´ë°±)
    _NON_LIFE_KW = [
        "í™”ì¬", "ì†í•´", "ì†ë³´", "ë‹¤ì´ë ‰íŠ¸", "ìºë¡¯", "í•˜ë‚˜ì†í•´",
    ]

    @classmethod
    def normalize(cls, name: str) -> str:
        return cls._ALIAS.get(name.strip(), name.strip())

    @classmethod
    def get(cls, company_name: str) -> Optional[dict]:
        normalized = cls.normalize(company_name)
        direct = cls._REG.get(normalized)
        if direct:
            return direct
        # ë¯¸ë“±ë¡ ë³´í—˜ì‚¬ â†’ ìƒëª…/ì†í•´ë³´í—˜í˜‘íšŒ ê³µì‹œì‹¤ í´ë°±
        if any(kw in normalized for kw in cls._NON_LIFE_KW):
            return cls._REG["ì†í•´ë³´í—˜í˜‘íšŒ"]
        return cls._REG["ìƒëª…ë³´í—˜í˜‘íšŒ"]

    @classmethod
    def all_companies(cls) -> list:
        return list(cls._REG.keys())


# ---------------------------------------------------------------------------
# 2. íŒë§¤ ê¸°ê°„ ë‚ ì§œ ë§¤ì²˜
# ---------------------------------------------------------------------------
class DateRangeMatcher:
    _DATE_PATS = [
        r"(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})",
        r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼",
        r"(\d{8})",
    ]
    _OPEN_KW = {"í˜„ì¬", "íŒë§¤ì¤‘", "íŒë§¤ ì¤‘"}

    @classmethod
    def _parse(cls, s: str) -> Optional[date]:
        for pat in cls._DATE_PATS:
            m = re.search(pat, s.strip())
            if not m:
                continue
            g = m.groups()
            if len(g) == 1:
                r = g[0]
                y, mo, d = int(r[:4]), int(r[4:6]), int(r[6:8])
            else:
                y, mo, d = int(g[0]), int(g[1]), int(g[2])
            try:
                return date(y, mo, d)
            except ValueError:
                continue
        return None

    @classmethod
    def is_date_in_period(cls, join_date_str: str, period_text: str) -> bool:
        jd = cls._parse(join_date_str)
        if not jd:
            return False
        sep = re.search(r"[~\-ï¼â€“â€”]", period_text)
        if not sep:
            return False
        left, right = period_text[:sep.start()], period_text[sep.start() + 1:]
        start = cls._parse(left)
        if not start:
            return False
        end_raw = right.strip()
        end = (date.today() if any(kw in end_raw for kw in cls._OPEN_KW) or not end_raw
               else (cls._parse(end_raw) or date.today()))
        return start <= jd <= end

    @classmethod
    def best_match(cls, join_date_str: str, candidates: list) -> Optional[dict]:
        # 1ìˆœìœ„: íŒë§¤ ê¸°ê°„ì´ ê°€ì…ì¼ì„ í¬í•¨í•˜ëŠ” í›„ë³´
        matched = [c for c in candidates if cls.is_date_in_period(join_date_str, c.get("period", ""))]
        if matched:
            matched.sort(
                key=lambda c: cls._parse(c.get("revision_date", "")) or date(1900, 1, 1),
                reverse=True,
            )
            return matched[0]
        # 2ìˆœìœ„: period ì—†ì§€ë§Œ revision_dateê°€ ê°€ì…ì¼ ì´ì „ì¸ í›„ë³´ (ê°œì •ì¼ ê¸°ì¤€ ìµœê·¼ ê²ƒ)
        jd = cls._parse(join_date_str)
        dated = [c for c in candidates
                 if not c.get("period") and cls._parse(c.get("revision_date", ""))]
        if dated and jd:
            before = [c for c in dated
                      if (cls._parse(c["revision_date"]) or date(9999,1,1)) <= jd]
            pool = before if before else dated
            pool.sort(key=lambda c: cls._parse(c.get("revision_date", "")) or date(1900,1,1),
                      reverse=True)
            return pool[0]
        return None


# ---------------------------------------------------------------------------
# 3. Playwright ê¸°ë°˜ ê³µì‹œì‹¤ í¬ë¡¤ëŸ¬
# ---------------------------------------------------------------------------
class PolicyDisclosureCrawler:
    """headless Playwrightë¡œ ê³µì‹œì‹¤ì„ íƒìƒ‰í•˜ì—¬ ì•½ê´€ PDF URL ì¶”ì¶œ."""

    _TIMEOUT_MS = 20_000
    _NAV_WAIT   = 2.0

    # Stealth: ë´‡ íƒì§€ ìš°íšŒìš© ì¶”ê°€ í—¤ë”/ì¸ì
    _STEALTH_ARGS = [
        "--disable-blink-features=AutomationControlled",
        "--no-sandbox",
        "--disable-dev-shm-usage",
    ]
    _STEALTH_HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Referer": "https://www.google.co.kr/",
    }

    def __init__(self, headless: bool = True):
        self.headless = headless

    def _launch(self) -> bool:
        self._launch_error = ""
        try:
            from playwright.sync_api import sync_playwright
            self._pw      = sync_playwright().__enter__()
            self._browser = self._pw.chromium.launch(
                headless=self.headless,
                args=self._STEALTH_ARGS,
            )
            return True
        except ImportError as e:
            self._launch_error = f"playwright ë¯¸ì„¤ì¹˜: {e}"
            return False
        except Exception as e:
            self._launch_error = f"Chromium ì‹¤í–‰ ì‹¤íŒ¨: {e}"
            return False

    def _close(self):
        try:
            self._browser.close()
            self._pw.__exit__(None, None, None)
        except Exception:
            pass

    def _safe_goto(self, page, url: str) -> bool:
        try:
            page.goto(url, timeout=self._TIMEOUT_MS, wait_until="domcontentloaded")
            time.sleep(self._NAV_WAIT)
            return True
        except Exception:
            return False

    # â”€â”€ í¼ì§€ ìƒí’ˆëª… ë§¤ì¹­ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë³´í—˜ ìƒí’ˆëª…ì—ì„œ ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´Â·ì¡°ì‚¬ë¥¼ ì œê±°í•˜ê³  í•µì‹¬ í† í°ë§Œ ì¶”ì¶œ
    _NOISE_WORDS = {
        "ë³´í—˜", "ë³´ì¥", "ê°€ì…", "íŠ¹ì•½", "í˜•", "(ë¬´)", "ë¬´ë°°ë‹¹", "ë°°ë‹¹",
        "the", "The", "NEW", "new", "Plus", "plus", "I", "II", "III",
        "1ì¢…", "2ì¢…", "3ì¢…", "ê°±ì‹ ", "ë¹„ê°±ì‹ ", "ì‹¤ì†", "ì‹¤ë¹„",
    }

    @classmethod
    def _core_tokens(cls, name: str) -> List[str]:
        """ìƒí’ˆëª…ì—ì„œ í•µì‹¬ í† í° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (2ì ì´ìƒ, ë…¸ì´ì¦ˆ ì œê±°)."""
        # ê´„í˜¸Â·íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ê³µë°± ë¶„ë¦¬
        cleaned = re.sub(r"[()\[\]ã€Šã€‹<>Â·â€¢]", " ", name)
        tokens  = [t.strip() for t in re.split(r"[\s_\-]+", cleaned) if t.strip()]
        core    = [t for t in tokens if len(t) >= 2 and t not in cls._NOISE_WORDS]
        return core if core else tokens  # ì „ë¶€ ë…¸ì´ì¦ˆë©´ ì›ë³¸ ë°˜í™˜

    @classmethod
    def _match_score(cls, product_name: str, row_text: str) -> float:
        """
        ìƒí’ˆëª… í•µì‹¬ í† í°ì´ row_textì— ëª‡ ê°œë‚˜ í¬í•¨ë˜ëŠ”ì§€ ë¹„ìœ¨(0~1) ë°˜í™˜.
        - ì™„ì „ í¬í•¨: 1.0 / í† í°
        - ë¶€ë¶„ í¬í•¨(row í† í°ì´ product í† í°ì„ í¬í•¨): 0.6 / í† í°
        ìµœì†Œ 1ê°œ í•µì‹¬ í† í° ë§¤ì¹­ ì‹œ í›„ë³´ë¡œ ì±„íƒ (ì„ê³„ê°’ 0.0 ì´ˆê³¼).
        """
        core = cls._core_tokens(product_name)
        if not core:
            return 0.0
        score = 0.0
        for tok in core:
            if tok in row_text:
                score += 1.0
            else:
                # ë¶€ë¶„ ë§¤ì¹­: í•œê¸€ 2ì ì´ìƒ ì ‘ë‘/ì ‘ë¯¸ í¬í•¨ ì—¬ë¶€
                if len(tok) >= 3 and any(tok[:i] in row_text for i in range(len(tok)-1, 1, -1)):
                    score += 0.6
        return score / len(core)

    @classmethod
    def _period_from_text(cls, text: str):
        return re.search(
            r"\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}\s*[~\-ï¼â€“â€”]\s*"
            r"(?:\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}|í˜„ì¬|íŒë§¤ì¤‘|íŒë§¤ ì¤‘)",
            text,
        )

    # ë§¤ì¹­ ì„ê³„ê°’ (0.0 ì´ˆê³¼ì´ë©´ í›„ë³´ í¬í•¨ â†’ í•µì‹¬ í† í° 1ê°œ ì´ìƒ ë§¤ì¹­)
    _MATCH_THRESHOLD = 0.0

    def _extract_candidates(self, page, product_name: str) -> list:
        scored: List[Tuple[float, dict]] = []

        # ì „ëµ 1: PDF ë§í¬ ì§ì ‘ ìˆ˜ì§‘
        try:
            links = page.query_selector_all("a[href$='.pdf'], a[href*='pdf'], a[href*='PDF']")
            for link in links:
                href = link.get_attribute("href") or ""
                if not href:
                    continue
                text = (link.inner_text() or "").strip()
                try:
                    row_text = str(link.evaluate(
                        "el => el.closest('tr')?.innerText || el.closest('li')?.innerText || ''"
                    )) or text
                except Exception:
                    row_text = text
                score = self._match_score(product_name, row_text)
                if score <= self._MATCH_THRESHOLD:
                    continue
                period_m = self._period_from_text(row_text)
                rev_m    = re.search(r"\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}", text)
                scored.append((score, {
                    "url": href, "text": text,
                    "period": period_m.group(0) if period_m else "",
                    "revision_date": rev_m.group(0) if rev_m else "",
                }))
        except Exception:
            pass

        # ì „ëµ 2: í–‰ ê¸°ë°˜ íƒìƒ‰ (JS ë Œë”ë§ ê³µì‹œì‹¤ ëŒ€ì‘)
        if not scored:
            try:
                rows = page.query_selector_all("tr, .list-item, .product-item")
                for row in rows:
                    row_text = row.inner_text() or ""
                    score = self._match_score(product_name, row_text)
                    if score <= self._MATCH_THRESHOLD:
                        continue
                    btn  = row.query_selector("a[href], button")
                    href = (btn.get_attribute("href") or "") if btn else ""
                    period_m = self._period_from_text(row_text)
                    scored.append((score, {
                        "url": href,
                        "text": (btn.inner_text()[:80] if btn else ""),
                        "period": period_m.group(0) if period_m else "",
                        "revision_date": "",
                    }))
            except Exception:
                pass

        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ í›„ë³´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        scored.sort(key=lambda x: x[0], reverse=True)
        return [c for _, c in scored]

    def _try_discontinued_tab(self, page, product_name: str) -> list:
        """íŒë§¤ì¤‘ì§€ íƒ­/ë²„íŠ¼ì„ ìë™ íƒìƒ‰í•˜ì—¬ í›„ë³´ ì¶”ì¶œ."""
        _DISC_KEYWORDS = [
            "íŒë§¤ì¤‘ì§€", "íŒë§¤ ì¤‘ì§€", "ê³¼ê±°ìƒí’ˆ", "ê³¼ê±° ìƒí’ˆ",
            "discontinued", "íŒë§¤ì¢…ë£Œ", "íŒë§¤ ì¢…ë£Œ",
        ]
        try:
            for kw in _DISC_KEYWORDS:
                btns = page.query_selector_all(
                    f"a, button, li, span, div"
                )
                for btn in btns:
                    try:
                        txt = (btn.inner_text() or "").strip()
                        if kw in txt and len(txt) < 30:
                            btn.click()
                            time.sleep(self._NAV_WAIT)
                            cands = self._extract_candidates(page, product_name)
                            if cands:
                                logger.info(f"íŒë§¤ì¤‘ì§€ íƒ­ '{txt}' í´ë¦­ìœ¼ë¡œ {len(cands)}ê°œ í›„ë³´ ë°œê²¬")
                                return cands
                    except Exception:
                        continue
        except Exception:
            pass
        return []

    @staticmethod
    def _resolve_url(base: str, href: str) -> str:
        if href.startswith("http"):
            return href
        from urllib.parse import urljoin
        return urljoin(base, href)

    def _fetch_meritz(self, page, product_name: str, join_date: str, base: str) -> list:
        """ë©”ë¦¬ì¸ í™”ì¬ SPA ê³µì‹œì‹¤ ì „ìš© íƒìƒ‰ (íŒë§¤ì¤‘ + íŒë§¤ì¤‘ì§€ íƒ­ ëª¨ë‘ ì‹œë„)."""
        _MERITZ_BASE = "https://www.meritzfire.com/disclosure/product-announcement/product-list.do"
        candidates = []
        for hash_frag in ["", "#!/02"]:   # íŒë§¤ì¤‘ íƒ­, íŒë§¤ì¤‘ì§€ íƒ­
            url = _MERITZ_BASE + hash_frag
            try:
                page.goto(url, timeout=self._TIMEOUT_MS, wait_until="networkidle")
                time.sleep(3.0)   # SPA ë Œë”ë§ ëŒ€ê¸°
            except Exception:
                continue
            # ê²€ìƒ‰ì°½ ì‹œë„
            try:
                inp = page.query_selector("input[type='text'], input[placeholder]")
                if inp:
                    inp.fill(product_name)
                    page.keyboard.press("Enter")
                    time.sleep(2.0)
            except Exception:
                pass
            cands = self._extract_candidates(page, product_name)
            candidates.extend(cands)
        return candidates

    def _fetch_core(self, company_name: str, product_name: str, join_date: str) -> dict:
        """ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë¨)."""
        info = CompanyUrlRegistry.get(company_name)
        if not info:
            return self._err(f"'{company_name}' ê³µì‹œì‹¤ ë¯¸ë“±ë¡")
        if not self._launch():
            return self._err(getattr(self, "_launch_error", None) or "Chromium ì‹¤í–‰ ì‹¤íŒ¨ (playwright install chromium --with-deps í•„ìš”)")

        res = dict(pdf_url="", period="", revision_date="",
                   confidence=0, reason="", candidates_count=0, error="")
        try:
            page = self._browser.new_page()
            page.set_extra_http_headers(self._STEALTH_HEADERS)
            page.add_init_script(
                "Object.defineProperty(navigator,'webdriver',{get:()=>undefined})"
            )

            # ë©”ë¦¬ì¸ í™”ì¬: SPA(í•´ì‹œ ë¼ìš°íŒ…) ê³µì‹œì‹¤ ì „ìš© íƒìƒ‰
            _normalized_co = CompanyUrlRegistry.normalize(company_name)
            if _normalized_co == "ë©”ë¦¬ì¸ í™”ì¬":
                candidates = self._fetch_meritz(page, product_name, join_date, info["base"])
            else:
                # í•µì‹¬ í† í° ì¤‘ ê°€ì¥ ê¸´ 1ê°œë¥¼ ê²€ìƒ‰ì–´ë¡œ ì‚¬ìš© (ë„ˆë¬´ ê¸´ ì´ë¦„ì€ ê²€ìƒ‰ íˆíŠ¸ìœ¨ â†“)
                _core = PolicyDisclosureCrawler._core_tokens(product_name)
                _search_q = max(_core, key=len) if _core else product_name
                search_url = f"{info['url']}?{info['p']}={requests.utils.quote(_search_q)}"
                if not self._safe_goto(page, search_url):
                    self._safe_goto(page, info["url"])
                    try:
                        page.fill(f"input[name='{info['p']}']", _search_q)
                        page.keyboard.press("Enter")
                        time.sleep(self._NAV_WAIT)
                    except Exception:
                        pass
                candidates = self._extract_candidates(page, product_name)
                # ê²°ê³¼ ì—†ìœ¼ë©´ í•µì‹¬ í† í° 2ê°œ ì¡°í•©ìœ¼ë¡œ ì¬ì‹œë„
                if not candidates and len(_core) >= 2:
                    _search_q2 = " ".join(_core[:2])
                    search_url2 = f"{info['url']}?{info['p']}={requests.utils.quote(_search_q2)}"
                    if self._safe_goto(page, search_url2):
                        candidates = self._extract_candidates(page, product_name)
                if not candidates:
                    candidates = self._try_discontinued_tab(page, product_name)

            res["candidates_count"] = len(candidates)

            if not candidates:
                res["error"]  = "ê³µì‹œì‹¤ì—ì„œ PDF ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                res["reason"] = "ìƒí’ˆëª… ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ë˜ëŠ” JS ë Œë”ë§ í•„ìš”"
                return res

            best = DateRangeMatcher.best_match(join_date, candidates)
            if best:
                res.update(
                    pdf_url       = self._resolve_url(info["base"], best["url"]),
                    period        = best["period"],
                    revision_date = best["revision_date"],
                    confidence    = 92,
                    reason        = (
                        f"íŒë§¤ ê¸°ê°„ '{best['period']}'ì´ ê°€ì…ì¼ì {join_date}ë¥¼ í¬í•¨. "
                        f"ê°œì •ì¼: {best['revision_date'] or 'ë¯¸í™•ì¸'}. "
                        f"ì´ {len(candidates)}ê°œ í›„ë³´ ì¤‘ ìµœì  ì„ íƒ."
                    ),
                )
            else:
                fb = candidates[0]
                res.update(
                    pdf_url    = self._resolve_url(info["base"], fb["url"]),
                    period     = fb["period"],
                    confidence = 40,
                    reason     = (
                        f"ê°€ì…ì¼ì {join_date} ë§¤ì¹­ ì‹¤íŒ¨. "
                        f"ì²« ë²ˆì§¸ ê²°ê³¼({fb['text'][:60]}) ë°˜í™˜. ìˆ˜ë™ í™•ì¸ ê¶Œì¥."
                    ),
                )
        except Exception as e:
            res["error"] = str(e)[:300]
        finally:
            self._close()
        return res

    def fetch(self, company_name: str, product_name: str, join_date: str) -> dict:
        """ê³µì‹œì‹¤ íƒìƒ‰ ì‹¤í–‰. Streamlit asyncio ë£¨í”„ì™€ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰."""
        try:
            with ThreadPoolExecutor(max_workers=1) as _ex:
                future = _ex.submit(self._fetch_core, company_name, product_name, join_date)
                return future.result(timeout=120)
        except FuturesTimeoutError:
            return self._err("í¬ë¡¤ë§ ì‹œê°„ ì´ˆê³¼ (120ì´ˆ)")
        except Exception as e:
            return self._err(f"í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")

    @staticmethod
    def _err(msg: str) -> dict:
        return dict(pdf_url="", period="", revision_date="",
                    confidence=0, reason=msg, candidates_count=0, error=msg)


# ---------------------------------------------------------------------------
# 4. JIT ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸
# ---------------------------------------------------------------------------
class JITPipelineRunner:
    """
    PDF URL â†’ pdfplumber ì²­í‚¹ â†’ Supabase gk_policy_terms í…Œì´ë¸” ì ì¬.

    í•„ìš” DDL (Supabase SQL Editorì—ì„œ 1íšŒ ì‹¤í–‰):
        CREATE TABLE IF NOT EXISTS gk_policy_terms (
            id           BIGSERIAL PRIMARY KEY,
            company      TEXT NOT NULL,
            product      TEXT NOT NULL,
            join_date    TEXT,
            pdf_url      TEXT,
            chunk_idx    INT  DEFAULT 0,
            chunk_text   TEXT NOT NULL,
            char_count   INT  DEFAULT 0,
            content_hash TEXT,
            indexed_at   TIMESTAMPTZ DEFAULT now()
        );
        CREATE UNIQUE INDEX IF NOT EXISTS idx_gk_policy_terms_hash
            ON gk_policy_terms(content_hash);
    """

    TABLE         = "gk_policy_terms"
    CHUNK_SIZE    = 800
    CHUNK_OVERLAP = 100

    def __init__(self, sb_client):
        self.sb = sb_client

    def _download_pdf(self, url: str) -> Optional[bytes]:
        try:
            resp = requests.get(url, headers={
                "User-Agent": "Mozilla/5.0 Chrome/120.0.0.0",
                "Accept": "application/pdf,*/*",
            }, timeout=30)
            resp.raise_for_status()
            return resp.content
        except Exception:
            return None

    def _pdf_to_chunks(self, pdf_bytes: bytes) -> list:
        full_text = ""
        # 1ì°¨: pdfplumber (í…ìŠ¤íŠ¸ PDF)
        try:
            import pdfplumber
            with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                full_text = "\n".join(p.extract_text() or "" for p in pdf.pages)
        except Exception:
            pass
        # 2ì°¨: pypdf fallback (ì•”í˜¸í™”/êµ¬í˜• PDF ëŒ€ì‘)
        if not full_text.strip():
            try:
                from pypdf import PdfReader
                reader = PdfReader(io.BytesIO(pdf_bytes))
                full_text = "\n".join(
                    (page.extract_text() or "") for page in reader.pages
                )
            except Exception:
                pass
        # 3ì°¨: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ì „ ì‹¤íŒ¨ ì‹œ ê²½ê³  ë¡œê·¸
        if not full_text.strip():
            logger.warning("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ â€” ì´ë¯¸ì§€ ì „ìš© PDFì´ê±°ë‚˜ ì•”í˜¸í™”ë¨")
            return []
        step = self.CHUNK_SIZE - self.CHUNK_OVERLAP
        return [
            full_text[i: i + self.CHUNK_SIZE].strip()
            for i in range(0, len(full_text), step)
            if len(full_text[i: i + self.CHUNK_SIZE].strip()) > 50
        ]

    def _upsert(self, company, product, join_date, pdf_url, idx, text,
                revision_date: str = "", period: str = "",
                storage_path: str = "") -> bool:
        h = hashlib.sha256(text.encode("utf-8", errors="replace")).hexdigest()
        if not self.sb:
            return False
        try:
            self.sb.table(self.TABLE).upsert(
                {"company": company, "product": product, "join_date": join_date,
                 "pdf_url": pdf_url, "chunk_idx": idx,
                 "chunk_text": text[:4000], "char_count": len(text),
                 "content_hash": h,
                 "revision_date": revision_date,
                 "sale_period": period,
                 "storage_path": storage_path,
                 "indexed_at": datetime.utcnow().isoformat()},
                on_conflict="content_hash",
            ).execute()
            return True
        except Exception:
            return False

    def is_cached(self, company: str, product: str, join_date: str) -> bool:
        if not self.sb:
            return False
        try:
            r = (self.sb.table(self.TABLE)
                 .select("id", count="exact")
                 .eq("company", company).eq("product", product).eq("join_date", join_date)
                 .limit(1).execute())
            return (r.count or 0) > 0
        except Exception:
            return False

    def run(self, company: str, product: str, join_date: str,
            pdf_url: str, progress_cb=None) -> dict:
        def _log(msg):
            if progress_cb:
                progress_cb(msg)

        res = dict(ok=False, chunks_indexed=0, chunks_failed=0,
                   pdf_bytes_size=0, pdf_bytes=None, storage_path="", error="")

        _log(f"ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {pdf_url[:80]}...")
        pdf_bytes = self._download_pdf(pdf_url)
        if not pdf_bytes:
            res["error"] = "PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. URL ì ‘ê·¼ ë¶ˆê°€ ë˜ëŠ” ë³´ì•ˆ ì°¨ë‹¨."
            _log(f"âŒ {res['error']}")
            return res

        res["pdf_bytes"] = pdf_bytes
        res["pdf_bytes_size"] = len(pdf_bytes)
        _log(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(pdf_bytes)//1024}KB). í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")

        # Supabase Storageì— ì›ë³¸ PDF ì €ì¥
        storage_path = self._save_pdf_to_storage(company, product, join_date, pdf_bytes)
        if storage_path:
            res["storage_path"] = storage_path
            _log(f"ğŸ’¾ ì›ë³¸ PDF ì €ì¥ë¨: {storage_path}")

        chunks = self._pdf_to_chunks(pdf_bytes)
        if not chunks:
            res["error"] = "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì´ë¯¸ì§€ PDF ë˜ëŠ” ì•”í˜¸í™”)."
            _log(f"âŒ {res['error']}")
            return res

        _log(f"ğŸ“„ {len(chunks)}ê°œ ì²­í¬ â†’ Supabase ì ì¬ ì¤‘...")
        for idx, chunk in enumerate(chunks):
            if self._upsert(company, product, join_date, pdf_url, idx, chunk,
                            storage_path=storage_path):
                res["chunks_indexed"] += 1
            else:
                res["chunks_failed"] += 1
            if progress_cb and idx % 10 == 0:
                progress_cb(f"  ì²­í¬ {idx+1}/{len(chunks)} ì ì¬...")

        res["ok"] = res["chunks_indexed"] > 0
        _log(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {res['chunks_indexed']}ê°œ ì„±ê³µ / {res['chunks_failed']}ê°œ ì‹¤íŒ¨")

        if not res["ok"]:
            _write_crawl_error_log(self.sb, company, product, join_date,
                                   pdf_url, "ì¸ë±ì‹± ì „ì²´ ì‹¤íŒ¨")
        return res

    def _save_pdf_to_storage(self, company: str, product: str,
                              join_date: str, pdf_bytes: bytes) -> str:
        """Supabase Storage 'policy-terms' ë²„í‚·ì— ì›ë³¸ PDF ì €ì¥. ê²½ë¡œ ë°˜í™˜."""
        if not self.sb or not pdf_bytes:
            return ""
        import re as _re
        safe = lambda s: _re.sub(r"[^\wê°€-í£]", "_", s)[:40]
        path = f"policy_terms/{safe(company)}/{safe(product)}_{join_date or 'unknown'}.pdf"
        try:
            self.sb.storage.from_("policy-terms").upload(
                path, pdf_bytes,
                file_options={"content-type": "application/pdf", "upsert": "true"},
            )
            return path
        except Exception:
            # ë²„í‚· ì—†ê±°ë‚˜ ê¶Œí•œ ì—†ìœ¼ë©´ ì¡°ìš©íˆ ë¬´ì‹œ
            return ""

    def search_terms(self, company: str, product: str, keyword: str, limit: int = 5) -> list:
        """ì¸ë±ì‹±ëœ ì•½ê´€ì—ì„œ í‚¤ì›Œë“œ ILIKE ê²€ìƒ‰"""
        if not self.sb:
            return []
        try:
            r = (self.sb.table(self.TABLE)
                 .select("chunk_text, chunk_idx, join_date, pdf_url")
                 .eq("company", company).eq("product", product)
                 .ilike("chunk_text", f"%{keyword}%")
                 .order("chunk_idx").limit(limit).execute())
            return r.data or []
        except Exception:
            return []


# ---------------------------------------------------------------------------
# 4-b. ì˜¤ë¥˜ ì•Œë¦¼ ì‹œìŠ¤í…œ (5ë²ˆ ê¸°ëŠ¥)
# ---------------------------------------------------------------------------
_ERROR_LOG_TABLE = "gk_crawl_error_log"
"""
DDL (Supabase SQL Editorì—ì„œ 1íšŒ ì‹¤í–‰):
    CREATE TABLE IF NOT EXISTS gk_crawl_error_log (
        id          BIGSERIAL PRIMARY KEY,
        company     TEXT,
        product     TEXT,
        join_date   TEXT,
        pdf_url     TEXT,
        error_msg   TEXT,
        logged_at   TIMESTAMPTZ DEFAULT now()
    );
"""

def _write_crawl_error_log(
    sb_client, company: str, product: str, join_date: str,
    pdf_url: str, error_msg: str
):
    """í¬ë¡¤ë§/ì¸ë±ì‹± ì‹¤íŒ¨ ì‹œ Supabase ì˜¤ë¥˜ ë¡œê·¸ í…Œì´ë¸”ì— ê¸°ë¡."""
    if not sb_client:
        logger.error(f"[CrawlError] {company}/{product}/{join_date}: {error_msg}")
        return
    try:
        sb_client.table(_ERROR_LOG_TABLE).insert({
            "company": company, "product": product,
            "join_date": join_date, "pdf_url": pdf_url,
            "error_msg": error_msg[:500],
            "logged_at": datetime.utcnow().isoformat(),
        }).execute()
    except Exception as e:
        logger.error(f"[CrawlError ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨] {e}")


def get_product_suggestions(sb_client, company: str = "", query: str = "",
                            limit: int = 20) -> list:
    """
    DBì— ì¸ë±ì‹±ëœ ìƒí’ˆëª… ëª©ë¡ì—ì„œ ìë™ì™„ì„± í›„ë³´ ë°˜í™˜.
    - company: ë³´í—˜ì‚¬ëª…ìœ¼ë¡œ í•„í„° (ë¹ˆ ë¬¸ìì—´ì´ë©´ ì „ì²´)
    - query: ì…ë ¥ì–´ í¬í•¨ í•„í„° (ë¹ˆ ë¬¸ìì—´ì´ë©´ ì „ì²´)
    ë°˜í™˜: [{"company": str, "product": str}, ...]
    """
    if not sb_client:
        return _BUILTIN_PRODUCT_DICT.get(company, []) if company else []
    try:
        q = (sb_client.table(JITPipelineRunner.TABLE)
             .select("company, product")
             .order("product"))
        if company:
            q = q.eq("company", company)
        if query:
            q = q.ilike("product", f"%{query}%")
        r = q.limit(limit * 5).execute()   # ì¤‘ë³µ ì œê±° ìœ„í•´ ë„‰ë„‰íˆ
        seen, result = set(), []
        for row in (r.data or []):
            key = (row["company"], row["product"])
            if key not in seen:
                seen.add(key)
                result.append({"company": row["company"], "product": row["product"]})
            if len(result) >= limit:
                break
        # DB ê²°ê³¼ ì—†ìœ¼ë©´ ë‚´ì¥ ì‚¬ì „ í´ë°±
        if not result:
            pool = _BUILTIN_PRODUCT_DICT.get(company, []) if company else [
                {"company": c, "product": p}
                for c, plist in _BUILTIN_PRODUCT_DICT.items()
                for p in plist
            ]
            if query:
                core = PolicyDisclosureCrawler._core_tokens(query)
                pool = [x for x in pool
                        if any(t in x["product"] for t in core)]
            return pool[:limit]
        return result
    except Exception:
        return []


# ë‚´ì¥ ìƒí’ˆëª… ì‚¬ì „ (DB ë¯¸ì—°ê²° ì‹œ í´ë°± / ì´ˆê¸° ìë™ì™„ì„± ì‹œë“œ)
_BUILTIN_PRODUCT_DICT: dict = {
    "ë©”ë¦¬ì¸ í™”ì¬": [
        "ë¬´ë°°ë‹¹ ë©”ë¦¬ì¸  ìš´ì „ìë³´í—˜", "ë¬´ë°°ë‹¹ ë©”ë¦¬ì¸  Theí–‰ë³µí•œ ìš´ì „ìë³´í—˜",
        "ë¬´ë°°ë‹¹ ì›°ìŠ¤ë¼ì´í”„ìš´ì „ìë³´í—˜", "ë¬´ë°°ë‹¹ ë©”ë¦¬ì¸  ì–´ë¦°ì´ë³´í—˜",
        "ë¬´ë°°ë‹¹ ë©”ë¦¬ì¸  ì•”ë³´í—˜", "ë¬´ë°°ë‹¹ ë©”ë¦¬ì¸  ì¢…í•©ë³´í—˜",
    ],
    "ì‚¼ì„±í™”ì¬": [
        "ë¬´ë°°ë‹¹ ì‚¼ì„± ì• ë‹ˆì¹´ ìš´ì „ìë³´í—˜", "ë¬´ë°°ë‹¹ ì‚¼ì„± New ìš´ì „ìë³´í—˜",
        "ë¬´ë°°ë‹¹ ì‚¼ì„± ì•”ë³´í—˜", "ë¬´ë°°ë‹¹ ì‚¼ì„± ì¢…í•©ë³´í—˜",
        "ë¬´ë°°ë‹¹ ì‚¼ì„± ì–´ë¦°ì´ë³´í—˜", "ë¬´ë°°ë‹¹ ì‚¼ì„± ê±´ê°•ë³´í—˜",
    ],
    "í˜„ëŒ€í•´ìƒ": [
        "ë¬´ë°°ë‹¹ í˜„ëŒ€ í•˜ì´ì¹´ ìš´ì „ìë³´í—˜", "ë¬´ë°°ë‹¹ í˜„ëŒ€í•´ìƒ ìš´ì „ìë³´í—˜",
        "ë¬´ë°°ë‹¹ í˜„ëŒ€ ì•”ë³´í—˜", "ë¬´ë°°ë‹¹ í˜„ëŒ€ ì¢…í•©ë³´í—˜",
    ],
    "DBì†í•´ë³´í—˜": [
        "ë¬´ë°°ë‹¹ DB ì°¸ì¢‹ì€ ìš´ì „ìë³´í—˜", "ë¬´ë°°ë‹¹ DB ì•”ë³´í—˜",
        "ë¬´ë°°ë‹¹ DB ì¢…í•©ë³´í—˜", "ë¬´ë°°ë‹¹ DB ì–´ë¦°ì´ë³´í—˜",
    ],
    "KBì†í•´ë³´í—˜": [
        "ë¬´ë°°ë‹¹ KB ê¸ˆìª½ê°™ì€ ìë…€ë³´í—˜", "ë¬´ë°°ë‹¹ KB ìš´ì „ìë³´í—˜",
        "ë¬´ë°°ë‹¹ KB ì•”ë³´í—˜", "ë¬´ë°°ë‹¹ KB ì¢…í•©ë³´í—˜",
    ],
    "í•œí™”ìƒëª…": [
        "ë¬´ë°°ë‹¹ í•œí™”ìƒëª… ìš´ì „ìë³´í—˜", "ë¬´ë°°ë‹¹ í•œí™” ì•”ë³´í—˜",
        "ë¬´ë°°ë‹¹ í•œí™” ì¢…ì‹ ë³´í—˜", "ë¬´ë°°ë‹¹ í•œí™” ê±´ê°•ë³´í—˜",
    ],
    "ì‚¼ì„±ìƒëª…": [
        "ë¬´ë°°ë‹¹ ì‚¼ì„±ìƒëª… ì•”ë³´í—˜", "ë¬´ë°°ë‹¹ ì‚¼ì„± ì¢…ì‹ ë³´í—˜",
        "ë¬´ë°°ë‹¹ ì‚¼ì„±ìƒëª… ê±´ê°•ë³´í—˜", "ë¬´ë°°ë‹¹ ì‚¼ì„±ìƒëª… ì¹˜ì•„ë³´í—˜",
    ],
    "êµë³´ìƒëª…": [
        "ë¬´ë°°ë‹¹ êµë³´ ì•”ë³´í—˜", "ë¬´ë°°ë‹¹ êµë³´ ì¢…ì‹ ë³´í—˜",
        "ë¬´ë°°ë‹¹ êµë³´ ê±´ê°•ë³´í—˜", "ë¬´ë°°ë‹¹ êµë³´ ì–´ë¦°ì´ë³´í—˜",
    ],
}


def get_crawl_error_logs(sb_client, limit: int = 50) -> list:
    """ê´€ë¦¬ììš©: ìµœê·¼ í¬ë¡¤ë§ ì˜¤ë¥˜ ë¡œê·¸ ì¡°íšŒ."""
    if not sb_client:
        return []
    try:
        r = (sb_client.table(_ERROR_LOG_TABLE)
             .select("*")
             .order("logged_at", desc=True)
             .limit(limit)
             .execute())
        return r.data or []
    except Exception:
        return []


# ---------------------------------------------------------------------------
# 5. í†µí•© JIT ì¡°íšŒ ì§„ì…ì  (app.pyì—ì„œ ë‹¨ì¼ í˜¸ì¶œ)
# ---------------------------------------------------------------------------
def run_jit_policy_lookup(
    company_name: str,
    product_name: str,
    join_date: str,
    sb_client,
    progress_cb=None,
) -> dict:
    """
    JIT ì•½ê´€ ì¡°íšŒ ì „ì²´ íŒŒì´í”„ë¼ì¸ ë‹¨ì¼ ì§„ì…ì .

    íë¦„:
      1. Supabase DB ìºì‹œ í™•ì¸ (ì´ë¯¸ ì¸ë±ì‹± â†’ ì¦‰ì‹œ ë°˜í™˜)
      2. ì—†ìœ¼ë©´ ê³µì‹œì‹¤ í¬ë¡¤ë§ â†’ PDF URL íšë“
      3. JIT ì¸ë±ì‹± â†’ Supabase ì ì¬

    ë°˜í™˜:
      {
        "cached": bool,
        "pdf_url": str,
        "period": str,
        "confidence": int,       # 0~100
        "reason": str,
        "chunks_indexed": int,
        "error": str,
      }
    """

    def _log(msg: str):
        if progress_cb:
            progress_cb(msg)

    pipeline = JITPipelineRunner(sb_client)
    result   = dict(cached=False, pdf_url="", period="",
                    confidence=0, reason="", chunks_indexed=0, error="")

    # â”€â”€ Step 1: ìºì‹œ í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if pipeline.is_cached(company_name, product_name, join_date):
        result.update(cached=True, confidence=100,
                      reason="Supabase DBì— ì´ë¯¸ ì¸ë±ì‹±ëœ ì•½ê´€ì…ë‹ˆë‹¤. ì¦‰ì‹œ ê²€ìƒ‰ ê°€ëŠ¥.")
        _log("âœ… DB ìºì‹œ íˆíŠ¸ â€” ê³µì‹œì‹¤ í¬ë¡¤ë§ ìƒëµ")
        return result

    _log(f"ğŸ” DB ë¯¸ë“±ë¡ â†’ ê³µì‹œì‹¤ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹œì‘ "
         f"({company_name} / {product_name} / ê°€ì…ì¼ {join_date})")

    # â”€â”€ Step 2: ê³µì‹œì‹¤ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    crawl = PolicyDisclosureCrawler().fetch(company_name, product_name, join_date)
    result.update(pdf_url=crawl["pdf_url"], period=crawl["period"],
                  confidence=crawl["confidence"], reason=crawl["reason"],
                  error=crawl["error"])

    if not crawl["pdf_url"]:
        _log(f"âŒ ê³µì‹œì‹¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl['error']}")
        # (5) ì˜¤ë¥˜ ì•Œë¦¼: í¬ë¡¤ë§ ì‹¤íŒ¨ ë¡œê·¸ ê¸°ë¡
        _write_crawl_error_log(
            sb_client, company_name, product_name, join_date,
            "", crawl["error"]
        )
        return result

    _log(f"âœ… ì•½ê´€ PDF í™•ë³´ (ì‹ ë¢°ë„ {crawl['confidence']}%) â†’ ì¸ë±ì‹± ì‹œì‘")

    # â”€â”€ Step 3: JIT ì¸ë±ì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pipe_res = pipeline.run(
        company_name, product_name, join_date, crawl["pdf_url"], progress_cb=_log
    )
    result["chunks_indexed"] = pipe_res["chunks_indexed"]
    result["pdf_bytes"]       = pipe_res.get("pdf_bytes")
    result["storage_path"]    = pipe_res.get("storage_path", "")
    if pipe_res["error"]:
        result["error"] = pipe_res["error"]

    return result


# ---------------------------------------------------------------------------
# 5-b. ìŠ¤ìº” ê²°ê³¼ ê¸°ë°˜ ì¼ê´„ JIT í¬ë¡¤ë§ (scan_hub ì—°ë™)
# ---------------------------------------------------------------------------

def run_batch_jit_from_scan(
    scan_policies: list,
    sb_client,
    progress_cb=None,
) -> list:
    """
    insurance_scan.extract_policies_from_scan() ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„
    ìºì‹œ ë¯¸ì¡´ì¬ ìƒí’ˆë§Œ ì„ íƒì ìœ¼ë¡œ JIT í¬ë¡¤ë§Â·ì¸ë±ì‹±.

    Args:
        scan_policies: [{"company","product","join_date","source_file",...}, ...]
        sb_client:     Supabase í´ë¼ì´ì–¸íŠ¸
        progress_cb:   ì§„í–‰ ë©”ì‹œì§€ ì½œë°± í•¨ìˆ˜ (ì„ íƒ)

    Returns: [
        {
            "source_file": str,
            "company":     str,
            "product":     str,
            "join_date":   str,
            "status":      "cached" | "indexed" | "failed" | "skipped",
            "pdf_url":     str,
            "chunks_indexed": int,
            "error":       str,
        }, ...
    ]
    """
    def _log(msg: str):
        if progress_cb:
            progress_cb(msg)
        else:
            logger.info(msg)

    results = []
    pipeline = JITPipelineRunner(sb_client)

    for idx, pol in enumerate(scan_policies):
        company   = pol.get("company", "").strip()
        product   = pol.get("product", "").strip()
        join_date = pol.get("join_date", "").strip()
        src_file  = pol.get("source_file", "")
        conf      = pol.get("confidence", 0)

        base = {"source_file": src_file, "company": company,
                "product": product, "join_date": join_date,
                "pdf_url": "", "chunks_indexed": 0, "error": ""}

        _log(f"\n[{idx+1}/{len(scan_policies)}] {company} / {product} ({join_date})")

        # ì¶”ì¶œ ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê±´ë„ˆëœ€
        if conf < 40 or not company or not product:
            _log(f"  âš ï¸ ì¶”ì¶œ ì‹ ë¢°ë„ ë¶€ì¡±({conf}%) â€” ê±´ë„ˆëœ€")
            base["status"] = "skipped"
            base["error"]  = f"ì¶”ì¶œ ì‹ ë¢°ë„ {conf}% (ë³´í—˜ì‚¬/ìƒí’ˆëª… í™•ì¸ í•„ìš”)"
            results.append(base)
            continue

        # ìºì‹œ í™•ì¸
        if pipeline.is_cached(company, product, join_date):
            _log(f"  ğŸ’¾ ì´ë¯¸ ì¸ë±ì‹±ë¨ â€” í¬ë¡¤ë§ ìƒëµ")
            base["status"] = "cached"
            results.append(base)
            continue

        # ê°€ì…ì¼ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ëŒ€ì²´
        if not join_date:
            join_date = datetime.utcnow().strftime("%Y-%m-%d")
            base["join_date"] = join_date
            _log(f"  â„¹ï¸ ê°€ì…ì¼ ë¯¸í™•ì¸ â†’ ì˜¤ëŠ˜ ë‚ ì§œ({join_date}) ì‚¬ìš©")

        # JIT í¬ë¡¤ë§ ì‹¤í–‰
        jit_res = run_jit_policy_lookup(
            company_name=company,
            product_name=product,
            join_date=join_date,
            sb_client=sb_client,
            progress_cb=_log,
        )

        base["pdf_url"]        = jit_res.get("pdf_url", "")
        base["chunks_indexed"] = jit_res.get("chunks_indexed", 0)
        base["pdf_bytes"]      = jit_res.get("pdf_bytes")
        base["storage_path"]   = jit_res.get("storage_path", "")
        base["error"]          = jit_res.get("error", "")

        if jit_res.get("cached"):
            base["status"] = "cached"
        elif jit_res.get("pdf_url") and jit_res.get("chunks_indexed", 0) > 0:
            base["status"] = "indexed"
        else:
            base["status"] = "failed"

        results.append(base)

    ok  = sum(1 for r in results if r["status"] in ("indexed", "cached"))
    fail = sum(1 for r in results if r["status"] == "failed")
    skip = sum(1 for r in results if r["status"] == "skipped")
    _log(f"\nâœ… ì¼ê´„ í¬ë¡¤ë§ ì™„ë£Œ â€” ì„±ê³µ/ìºì‹œ: {ok}ê±´ | ì‹¤íŒ¨: {fail}ê±´ | ê±´ë„ˆëœ€: {skip}ê±´")
    return results


# ---------------------------------------------------------------------------
# 6. í•©ì„± ë°ì´í„° ìƒì„± (Synthetic QA Generator) â€” Gemini ê¸°ë°˜
# ---------------------------------------------------------------------------

# í•µì‹¬ ì¡°í•­ ì„¹ì…˜ í‚¤ì›Œë“œ (SDG ì§‘ì¤‘ ëŒ€ìƒ)
_CORE_SECTION_KEYWORDS = [
    "ë³´ìƒí•˜ëŠ” ì†í•´", "ë³´ìƒí•˜ì§€ ì•ŠëŠ” ì†í•´", "ë©´ì±…", "ë©´ì±…ì‚¬í•­", "ë©´ì±…ì¡°í•­",
    "ì§€ê¸‰ ì‚¬ìœ ", "ì§€ê¸‰ì‚¬ìœ ", "ë³´í—˜ê¸ˆ ì§€ê¸‰", "ì§€ê¸‰ ê¸°ì¤€", "ì§€ê¸‰ê¸°ì¤€",
    "ë³´ì¥ë‚´ìš©", "ë³´ì¥ ë‚´ìš©", "ë‹´ë³´", "íŠ¹ì•½", "ì£¼ìš” ë³´ì¥",
    "ì§„ë‹¨", "ìˆ˜ìˆ ", "ì…ì›", "í†µì›", "ì¬í™œ",
    "ê³ ì§€ì˜ë¬´", "ê³„ì•½ ì „ ì•Œë¦´ ì˜ë¬´", "ì•Œë¦´ ì˜ë¬´",
    "ë³´í—˜ë£Œ ë‚©ì…", "ë‚©ì…ë©´ì œ", "ì‹¤íš¨", "í•´ì§€", "í™˜ê¸‰",
]


class SyntheticQAGenerator:
    """
    ë³´í—˜ ì•½ê´€ í…ìŠ¤íŠ¸ â†’ Gemini Flash(ì €ë ´)ë¡œ í•µì‹¬ ì¡°í•­ ì„ ë³„
    â†’ Gemini Pro(ê³ ì„±ëŠ¥)ë¡œ í•©ì„± QA 20ê°œ ìƒì„±
    â†’ Supabase gk_policy_terms_qa í…Œì´ë¸”ì— ì›ë¬¸+QA ë³‘ë ¬ ì ì¬.

    DDL (Supabase SQL Editorì—ì„œ 1íšŒ ì‹¤í–‰):
        CREATE TABLE IF NOT EXISTS gk_policy_terms_qa (
            id            BIGSERIAL PRIMARY KEY,
            company       TEXT NOT NULL,
            product       TEXT NOT NULL,
            join_date     TEXT,
            section_type  TEXT,        -- 'original' | 'synthetic_q' | 'synthetic_qa'
            chunk_idx     INT  DEFAULT 0,
            chunk_text    TEXT NOT NULL,
            char_count    INT  DEFAULT 0,
            content_hash  TEXT,
            indexed_at    TIMESTAMPTZ DEFAULT now()
        );
        CREATE UNIQUE INDEX IF NOT EXISTS idx_gk_policy_terms_qa_hash
            ON gk_policy_terms_qa(content_hash);
    """

    TABLE_QA    = "gk_policy_terms_qa"
    CHUNK_SIZE  = 600
    MAX_CHUNKS_FOR_SDG = 30   # SDG ëŒ€ìƒ ì²­í¬ ìµœëŒ€ ìˆ˜ (ë¹„ìš© ì œì–´)

    # í•µì‹¬ ì¡°í•­ ì„¹ì…˜ ê°ì§€ (CORE_SECTION_KEYWORDS)
    _CORE_KW = _CORE_SECTION_KEYWORDS

    # Gemini ëª¨ë¸ ì„¤ì •
    # - ì§ˆë¬¸ ìƒì„±(ê³ ì„±ëŠ¥): gemini-2.0-flash  â† GPT-4o ëŒ€ì‹  Gemini ê³„ì—´ ìµœê³ ì„±ëŠ¥
    # - ì €ì¥/ê²€ìƒ‰ìš© ì„ë² ë”©: í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì €ì¥ (sentence-transformersëŠ” JITPipelineì´ ë‹´ë‹¹)
    MODEL_SDG   = "gemini-2.0-flash"       # í•©ì„± ì§ˆë¬¸ ìƒì„± (ê³ í’ˆì§ˆ)
    MODEL_CHEAP = "gemini-2.0-flash-lite"  # í•µì‹¬ ì¡°í•­ ì„ ë³„ (ì €ë¹„ìš©)

    def __init__(self, sb_client, gemini_client=None):
        self.sb     = sb_client
        self._gc    = gemini_client   # google.genai client (ì—†ìœ¼ë©´ ìƒëµ)

    # â”€â”€ í•µì‹¬ ì¡°í•­ ê°ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _is_core_section(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì— í•µì‹¬ ì¡°í•­ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True"""
        return any(kw in text for kw in self._CORE_KW)

    # â”€â”€ í•©ì„± QA ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _generate_qa(self, chunk_text: str, company: str, product: str) -> list:
        """
        Geminië¡œ ì•½ê´€ ì²­í¬ â†’ ì˜ˆìƒ ì§ˆë¬¸ 20ê°œ ìƒì„±.
        ë°˜í™˜: ["ì§ˆë¬¸1", "ì§ˆë¬¸2", ...]
        """
        if not self._gc:
            return []

        prompt = (
            f"ë„ˆëŠ” ë² í…Œë‘ ë³´í—˜ ì„¤ê³„ì‚¬ì•¼. ë‹¤ìŒ ë³´í—˜ ì•½ê´€ ë‚´ìš©ì„ ë³´ê³ , "
            f"ê³ ê°ë“¤ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ì§ˆë¬¸ 20ê°œë¥¼ ë§Œë“¤ì–´ì¤˜.\n\n"
            f"[ë³´í—˜ì‚¬] {company}\n"
            f"[ìƒí’ˆëª…] {product}\n\n"
            f"[ì•½ê´€ ë‚´ìš©]\n{chunk_text[:1200]}\n\n"
            f"[ìš”êµ¬ì‚¬í•­]\n"
            f"- ì§ˆë¬¸ì€ \"ì•” ì§„ë‹¨ ì‹œ ì–¼ë§ˆë¥¼ ë°›ë‚˜ìš”?\"ì™€ ê°™ì´ êµ¬ì–´ì²´ë¡œ ì‘ì„±í•  ê²ƒ.\n"
            f"- ë³´í—˜ê¸ˆ ì§€ê¸‰ ì¡°ê±´, ë©´ì±… ì¡°í•­, íŠ¹ì•½ ì‚¬í•­ì— ì§‘ì¤‘í•  ê²ƒ.\n"
            f"- ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œ ì¤„ì— í•˜ë‚˜ì”© ì§ˆë¬¸ë§Œ ë‚˜ì—´í•  ê²ƒ. ë²ˆí˜¸Â·ê¸°í˜¸ ì—†ì´ ì§ˆë¬¸ í…ìŠ¤íŠ¸ë§Œ.\n"
            f"- í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•  ê²ƒ."
        )
        try:
            from google.genai import types as _gt
            resp = self._gc.models.generate_content(
                model=self.MODEL_SDG,
                contents=prompt,
                config=_gt.GenerateContentConfig(
                    temperature=0.7,
                    max_output_tokens=1024,
                ),
            )
            raw = (resp.text or "").strip()
            questions = [
                ln.strip().lstrip("0123456789.-) ").strip()
                for ln in raw.split("\n")
                if ln.strip() and len(ln.strip()) > 5
            ]
            return questions[:20]
        except Exception:
            return []

    # â”€â”€ Supabase upsert â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _upsert_qa(self, company: str, product: str, join_date: str,
                   section_type: str, idx: int, text: str) -> bool:
        h = hashlib.sha256(text.encode("utf-8", errors="replace")).hexdigest()
        if not self.sb:
            return False
        try:
            self.sb.table(self.TABLE_QA).upsert(
                {"company": company, "product": product, "join_date": join_date,
                 "section_type": section_type, "chunk_idx": idx,
                 "chunk_text": text[:4000], "char_count": len(text),
                 "content_hash": h},
                on_conflict="content_hash",
            ).execute()
            return True
        except Exception:
            return False

    # â”€â”€ ë©”ì¸ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run(self, company: str, product: str, join_date: str,
            chunks: list, progress_cb=None) -> dict:
        """
        chunks: JITPipelineRunner._pdf_to_chunks() ê²°ê³¼ (str ë¦¬ìŠ¤íŠ¸)
        íë¦„:
          1. ì›ë¬¸ ì²­í¬ â†’ gk_policy_terms_qa ì €ì¥ (section_type='original')
          2. í•µì‹¬ ì¡°í•­ ì„ ë³„ (CORE_SECTION_KEYWORDS í¬í•¨ ì²­í¬ë§Œ)
          3. ì„ ë³„ëœ ì²­í¬ â†’ Gemini SDG â†’ í•©ì„± QA ìƒì„±
          4. "ì§ˆë¬¸\në‹µë³€ê·¼ê±°: ì›ë¬¸" í˜•íƒœë¡œ gk_policy_terms_qa ì €ì¥ (section_type='synthetic_qa')
        """
        def _log(msg: str):
            if progress_cb:
                progress_cb(msg)

        res = dict(original_saved=0, core_chunks=0,
                   qa_generated=0, qa_saved=0, error="")

        # Step 1: ì›ë¬¸ ì €ì¥
        _log("ğŸ“„ ì›ë¬¸ ì²­í¬ ì €ì¥ ì¤‘...")
        for idx, chunk in enumerate(chunks):
            if self._upsert_qa(company, product, join_date, "original", idx, chunk):
                res["original_saved"] += 1

        # Step 2: í•µì‹¬ ì¡°í•­ ì„ ë³„
        core_chunks = [c for c in chunks if self._is_core_section(c)]
        core_chunks  = core_chunks[:self.MAX_CHUNKS_FOR_SDG]
        res["core_chunks"] = len(core_chunks)
        _log(f"ğŸ¯ í•µì‹¬ ì¡°í•­ {len(core_chunks)}ê°œ ì²­í¬ ì„ ë³„ ì™„ë£Œ (SDG ëŒ€ìƒ)")

        if not core_chunks:
            _log("â„¹ï¸ í•µì‹¬ ì¡°í•­ í‚¤ì›Œë“œ ë¯¸í¬í•¨ â€” SDG ìƒëµ. ì›ë¬¸ë§Œ ì €ì¥ë¨.")
            return res

        if not self._gc:
            _log("âš ï¸ Gemini í´ë¼ì´ì–¸íŠ¸ ë¯¸ì—°ê²° â€” SDG ìƒëµ. ì›ë¬¸ë§Œ ì €ì¥ë¨.")
            return res

        # Step 3 & 4: SDG ì‹¤í–‰
        _log(f"ğŸ¤– Gemini({self.MODEL_SDG}) SDG ì‹œì‘ â€” í•µì‹¬ {len(core_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬...")
        qa_idx = len(chunks)   # ì›ë¬¸ ì´í›„ idx ë¶€í„° ì‹œì‘
        for c_idx, chunk in enumerate(core_chunks):
            _log(f"  [{c_idx+1}/{len(core_chunks)}] í•©ì„± ì§ˆë¬¸ ìƒì„± ì¤‘...")
            questions = self._generate_qa(chunk, company, product)
            res["qa_generated"] += len(questions)

            for q in questions:
                combined = f"ì§ˆë¬¸: {q}\në‹µë³€ê·¼ê±°: {chunk[:600]}"
                if self._upsert_qa(company, product, join_date,
                                   "synthetic_qa", qa_idx, combined):
                    res["qa_saved"] += 1
                    qa_idx += 1

        _log(
            f"âœ… SDG ì™„ë£Œ: ì›ë¬¸ {res['original_saved']}ê°œ + "
            f"í•©ì„± QA {res['qa_saved']}ê°œ ì €ì¥"
        )
        return res

    def search_semantic(self, company: str, product: str,
                        keyword: str, limit: int = 5,
                        include_synthetic: bool = True) -> list:
        """
        gk_policy_terms_qaì—ì„œ ILIKE ê²€ìƒ‰.
        include_synthetic=Trueë©´ ì›ë¬¸+í•©ì„±QA ëª¨ë‘, Falseë©´ ì›ë¬¸ë§Œ.
        """
        if not self.sb:
            return []
        try:
            q = (self.sb.table(self.TABLE_QA)
                 .select("chunk_text, chunk_idx, section_type, join_date")
                 .eq("company", company).eq("product", product)
                 .ilike("chunk_text", f"%{keyword}%")
                 .order("section_type").order("chunk_idx")
                 .limit(limit))
            if not include_synthetic:
                q = q.eq("section_type", "original")
            return q.execute().data or []
        except Exception:
            return []
