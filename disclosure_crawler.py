# ==========================================================================
# [ë³´í—˜ ê³µì‹œì‹¤ ì‹¤ì‹œê°„ ì•½ê´€ í¬ë¡¤ëŸ¬] disclosure_crawler.py
#
# ì—­í• : ë³´í—˜ì‚¬ ìƒí’ˆê³µì‹œì‹¤ì—ì„œ íŠ¹ì • ìƒí’ˆÂ·ê°€ì…ì¼ìì— ë§ëŠ” ì•½ê´€ PDFë¥¼
#       ì‹¤ì‹œê°„ íƒìƒ‰Â·ë‹¤ìš´ë¡œë“œÂ·RAG ì¸ë±ì‹±í•˜ëŠ” JIT(Just-in-Time) íŒŒì´í”„ë¼ì¸.
#
# ì„¤ì¹˜:
#   pip install playwright pdfplumber requests
#   playwright install chromium
# ==========================================================================

import io, re, time, hashlib, requests, logging
from datetime import date, datetime
from typing import Optional

logger = logging.getLogger("disclosure_crawler")


# ---------------------------------------------------------------------------
# 0. AI í¬ë¡¤ë§ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ---------------------------------------------------------------------------
CRAWLER_SYSTEM_PROMPT = """[ì—­í• ] ë„ˆëŠ” ë³´í—˜ ì•½ê´€ ì „ë¬¸ í¬ë¡¤ë§ ë° ì¸ë±ì‹± ì—ì´ì „íŠ¸ì•¼.

[ì„ë¬´]
1. ì‚¬ìš©ìê°€ ì…ë ¥í•œ [ë³´í—˜ì‚¬ëª…, ìƒí’ˆëª…, ê°€ì…ì¼ì]ë¥¼ í™•ì¸í•´.
2. í•´ë‹¹ ë³´í—˜ì‚¬ì˜ 'ìƒí’ˆê³µì‹œì‹¤' URL ê·œì¹™ì„ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´.
3. íŒë§¤ ê¸°ê°„(íŒë§¤ê°œì‹œì¼ ~ íŒë§¤ì¢…ë£Œì¼)ì´ ê°€ì…ì¼ìë¥¼ í¬í•¨í•˜ëŠ” PDFë§Œ ì„ íƒí•´.
4. ì—¬ëŸ¬ ë²„ì „: (a) íŒë§¤ ê¸°ê°„ í¬í•¨ ìš°ì„  (b) ë™ì¼ ì¡°ê±´ì´ë©´ ê°œì •ì¼ ìµœì‹  ì„ íƒ.
5. ë‹¤ìš´ë¡œë“œëœ PDFë¥¼ JIT RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¦‰ì‹œ ì „ë‹¬í•´.

[ì¶œë ¥] ì„ íƒ PDF URL / íŒë§¤ ê¸°ê°„ / ê°œì •ì¼ / ì‹ ë¢°ë„(0~100%) / ì„ íƒ ê·¼ê±°

[ê¸ˆì§€] ê°€ì…ì¼ ë²”ìœ„ ì™¸ ì•½ê´€ / ë¹„ê³µì‹ ê²½ë¡œ(ë¸”ë¡œê·¸Â·2ì°¨ì‚¬ì´íŠ¸) / ì•½ê´€ ì„ì˜ ìˆ˜ì •"""


# ---------------------------------------------------------------------------
# 1. ë³´í—˜ì‚¬ë³„ ê³µì‹œì‹¤ URL ë ˆì§€ìŠ¤íŠ¸ë¦¬
# ---------------------------------------------------------------------------
class CompanyUrlRegistry:
    _REG: dict = {
        "ì‚¼ì„±ìƒëª…":    {"base": "https://www.samsunglife.com",    "url": "https://www.samsunglife.com/customer/publicInfo/productDisclosure.do",    "p": "searchKeyword"},
        "í•œí™”ìƒëª…":    {"base": "https://www.hanwhalife.com",      "url": "https://www.hanwhalife.com/cust/disclosure/productlist.do",               "p": "searchWord"},
        "êµë³´ìƒëª…":    {"base": "https://www.kyobo.co.kr",         "url": "https://www.kyobo.co.kr/prd/disclosures/productDisclosure",              "p": "keyword"},
        "ì‹ í•œë¼ì´í”„":  {"base": "https://www.shinhanlife.co.kr",   "url": "https://www.shinhanlife.co.kr/hp/cdha0100.do",                           "p": "searchWord"},
        "NHë†í˜‘ìƒëª…":  {"base": "https://www.nhlife.co.kr",        "url": "https://www.nhlife.co.kr/disclosure/product",                            "p": "searchText"},
        "ë¯¸ë˜ì—ì…‹ìƒëª…":{"base": "https://life.miraeasset.com",     "url": "https://life.miraeasset.com/csc/disclosure/productTerms.do",             "p": "searchWord"},
        "DBìƒëª…":      {"base": "https://www.db-life.com",         "url": "https://www.db-life.com/customer/publicInfo/product.do",                 "p": "keyword"},
        "ì‚¼ì„±í™”ì¬":    {"base": "https://www.samsungfire.com",     "url": "https://www.samsungfire.com/cust/disclosure/productDisclosure.do",        "p": "searchKeyword"},
        "í˜„ëŒ€í•´ìƒ":    {"base": "https://www.hi.co.kr",            "url": "https://www.hi.co.kr/cms/disclosure/product/list.do",                    "p": "searchKeyword"},
        "DBì†í•´ë³´í—˜":  {"base": "https://www.idb.co.kr",           "url": "https://www.idb.co.kr/cust/disclosure/product.do",                       "p": "keyword"},
        "KBì†í•´ë³´í—˜":  {"base": "https://www.kbinsure.co.kr",      "url": "https://www.kbinsure.co.kr/cust/disclosure/product.do",                  "p": "searchWord"},
        "ë©”ë¦¬ì¸ í™”ì¬":  {"base": "https://www.meritzfire.com",      "url": "https://www.meritzfire.com/cust/disclosure/product.do",                  "p": "searchKeyword"},
        "ë¡¯ë°ì†í•´ë³´í—˜":{"base": "https://www.lotteins.co.kr",      "url": "https://www.lotteins.co.kr/cust/disclosure/product.do",                  "p": "keyword"},
        "í•œí™”ì†í•´ë³´í—˜":{"base": "https://www.hwgeneralins.com",    "url": "https://www.hwgeneralins.com/cust/disclosure/product.do",                "p": "keyword"},
        "í¥êµ­í™”ì¬":    {"base": "https://www.heungkukfire.co.kr",  "url": "https://www.heungkukfire.co.kr/cust/disclosure/product.do",              "p": "keyword"},
        "ìƒëª…ë³´í—˜í˜‘íšŒ":{"base": "https://klia.or.kr",              "url": "https://klia.or.kr/consumer/publicRelation/productDisclosure.do",        "p": "searchKeyword"},
        "ì†í•´ë³´í—˜í˜‘íšŒ":{"base": "https://www.knia.or.kr",          "url": "https://www.knia.or.kr/consumer/publicRelation/productDisclosure.do",    "p": "searchKeyword"},
    }
    _ALIAS: dict = {
        "ì‚¼ì„±": "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±í™”ì¬ë³´í—˜": "ì‚¼ì„±í™”ì¬",
        "í˜„ëŒ€": "í˜„ëŒ€í•´ìƒ", "í˜„ëŒ€í•´ìƒí™”ì¬": "í˜„ëŒ€í•´ìƒ",
        "DB": "DBì†í•´ë³´í—˜", "ë™ë¶€í™”ì¬": "DBì†í•´ë³´í—˜", "ë™ë¶€ìƒëª…": "DBìƒëª…",
        "KB": "KBì†í•´ë³´í—˜", "í•œí™”": "í•œí™”ìƒëª…",
        "ë†í˜‘": "NHë†í˜‘ìƒëª…", "ë¯¸ë˜ì—ì…‹": "ë¯¸ë˜ì—ì…‹ìƒëª…", "ë©”ë¦¬ì¸ ": "ë©”ë¦¬ì¸ í™”ì¬",
    }

    @classmethod
    def normalize(cls, name: str) -> str:
        return cls._ALIAS.get(name.strip(), name.strip())

    @classmethod
    def get(cls, company_name: str) -> Optional[dict]:
        return cls._REG.get(cls.normalize(company_name))

    @classmethod
    def all_companies(cls) -> list:
        return list(cls._REG.keys())


# ---------------------------------------------------------------------------
# 2. íŒë§¤ ê¸°ê°„ ë‚ ì§œ ë§¤ì²˜
# ---------------------------------------------------------------------------
class DateRangeMatcher:
    _DATE_PATS = [
        r"(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})",
        r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼",
        r"(\d{8})",
    ]
    _OPEN_KW = {"í˜„ì¬", "íŒë§¤ì¤‘", "íŒë§¤ ì¤‘"}

    @classmethod
    def _parse(cls, s: str) -> Optional[date]:
        for pat in cls._DATE_PATS:
            m = re.search(pat, s.strip())
            if not m:
                continue
            g = m.groups()
            if len(g) == 1:
                r = g[0]
                y, mo, d = int(r[:4]), int(r[4:6]), int(r[6:8])
            else:
                y, mo, d = int(g[0]), int(g[1]), int(g[2])
            try:
                return date(y, mo, d)
            except ValueError:
                continue
        return None

    @classmethod
    def is_date_in_period(cls, join_date_str: str, period_text: str) -> bool:
        jd = cls._parse(join_date_str)
        if not jd:
            return False
        sep = re.search(r"[~\-ï¼â€“â€”]", period_text)
        if not sep:
            return False
        left, right = period_text[:sep.start()], period_text[sep.start() + 1:]
        start = cls._parse(left)
        if not start:
            return False
        end_raw = right.strip()
        end = (date.today() if any(kw in end_raw for kw in cls._OPEN_KW) or not end_raw
               else (cls._parse(end_raw) or date.today()))
        return start <= jd <= end

    @classmethod
    def best_match(cls, join_date_str: str, candidates: list) -> Optional[dict]:
        matched = [c for c in candidates if cls.is_date_in_period(join_date_str, c.get("period", ""))]
        if not matched:
            return None
        matched.sort(
            key=lambda c: cls._parse(c.get("revision_date", "")) or date(1900, 1, 1),
            reverse=True,
        )
        return matched[0]


# ---------------------------------------------------------------------------
# 3. Playwright ê¸°ë°˜ ê³µì‹œì‹¤ í¬ë¡¤ëŸ¬
# ---------------------------------------------------------------------------
class PolicyDisclosureCrawler:
    """headless Playwrightë¡œ ê³µì‹œì‹¤ì„ íƒìƒ‰í•˜ì—¬ ì•½ê´€ PDF URL ì¶”ì¶œ."""

    _TIMEOUT_MS = 20_000
    _NAV_WAIT   = 2.0

    # Stealth: ë´‡ íƒì§€ ìš°íšŒìš© ì¶”ê°€ í—¤ë”/ì¸ì
    _STEALTH_ARGS = [
        "--disable-blink-features=AutomationControlled",
        "--no-sandbox",
        "--disable-dev-shm-usage",
    ]
    _STEALTH_HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Referer": "https://www.google.co.kr/",
    }

    def __init__(self, headless: bool = True):
        self.headless = headless

    def _launch(self) -> bool:
        try:
            from playwright.sync_api import sync_playwright
            self._pw      = sync_playwright().__enter__()
            self._browser = self._pw.chromium.launch(
                headless=self.headless,
                args=self._STEALTH_ARGS,
            )
            return True
        except ImportError:
            return False

    def _close(self):
        try:
            self._browser.close()
            self._pw.__exit__(None, None, None)
        except Exception:
            pass

    def _safe_goto(self, page, url: str) -> bool:
        try:
            page.goto(url, timeout=self._TIMEOUT_MS, wait_until="domcontentloaded")
            time.sleep(self._NAV_WAIT)
            return True
        except Exception:
            return False

    def _extract_candidates(self, page, product_name: str) -> list:
        candidates = []
        keywords   = [kw for kw in product_name.split() if len(kw) >= 2]

        # ì „ëµ 1: PDF ë§í¬ ì§ì ‘ ìˆ˜ì§‘
        try:
            links = page.query_selector_all("a[href$='.pdf'], a[href*='pdf'], a[href*='PDF']")
            for link in links:
                href = link.get_attribute("href") or ""
                text = (link.inner_text() or "").strip()
                try:
                    row_text = str(link.evaluate(
                        "el => el.closest('tr')?.innerText || el.closest('li')?.innerText || ''"
                    ))
                except Exception:
                    row_text = text
                if not any(kw in row_text for kw in keywords):
                    continue
                period_m = re.search(
                    r"\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}\s*[~\-ï¼â€“â€”]\s*"
                    r"(?:\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}|í˜„ì¬|íŒë§¤ì¤‘|íŒë§¤ ì¤‘)",
                    row_text,
                )
                rev_m = re.search(r"\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}", text)
                if href:
                    candidates.append({
                        "url": href, "text": text,
                        "period": period_m.group(0) if period_m else "",
                        "revision_date": rev_m.group(0) if rev_m else "",
                    })
        except Exception:
            pass

        # ì „ëµ 2: í–‰ ê¸°ë°˜ íƒìƒ‰ (JS ë Œë”ë§ ê³µì‹œì‹¤ ëŒ€ì‘)
        if not candidates:
            try:
                rows = page.query_selector_all("tr, .list-item, .product-item")
                for row in rows:
                    row_text = row.inner_text() or ""
                    if not any(kw in row_text for kw in keywords):
                        continue
                    btn  = row.query_selector("a[href], button")
                    href = (btn.get_attribute("href") or "") if btn else ""
                    period_m = re.search(
                        r"\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}\s*[~\-ï¼â€“â€”]\s*"
                        r"(?:\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}|í˜„ì¬|íŒë§¤ì¤‘)",
                        row_text,
                    )
                    candidates.append({
                        "url": href,
                        "text": (btn.inner_text()[:80] if btn else ""),
                        "period": period_m.group(0) if period_m else "",
                        "revision_date": "",
                    })
            except Exception:
                pass

        return candidates

    def _try_discontinued_tab(self, page, product_name: str) -> list:
        """íŒë§¤ì¤‘ì§€ íƒ­/ë²„íŠ¼ì„ ìë™ íƒìƒ‰í•˜ì—¬ í›„ë³´ ì¶”ì¶œ."""
        _DISC_KEYWORDS = [
            "íŒë§¤ì¤‘ì§€", "íŒë§¤ ì¤‘ì§€", "ê³¼ê±°ìƒí’ˆ", "ê³¼ê±° ìƒí’ˆ",
            "discontinued", "íŒë§¤ì¢…ë£Œ", "íŒë§¤ ì¢…ë£Œ",
        ]
        try:
            for kw in _DISC_KEYWORDS:
                btns = page.query_selector_all(
                    f"a, button, li, span, div"
                )
                for btn in btns:
                    try:
                        txt = (btn.inner_text() or "").strip()
                        if kw in txt and len(txt) < 30:
                            btn.click()
                            time.sleep(self._NAV_WAIT)
                            cands = self._extract_candidates(page, product_name)
                            if cands:
                                logger.info(f"íŒë§¤ì¤‘ì§€ íƒ­ '{txt}' í´ë¦­ìœ¼ë¡œ {len(cands)}ê°œ í›„ë³´ ë°œê²¬")
                                return cands
                    except Exception:
                        continue
        except Exception:
            pass
        return []

    @staticmethod
    def _resolve_url(base: str, href: str) -> str:
        if href.startswith("http"):
            return href
        from urllib.parse import urljoin
        return urljoin(base, href)

    def fetch(self, company_name: str, product_name: str, join_date: str) -> dict:
        """ê³µì‹œì‹¤ íƒìƒ‰ ì‹¤í–‰. ë°˜í™˜: {pdf_url, period, revision_date, confidence, reason, candidates_count, error}"""
        info = CompanyUrlRegistry.get(company_name)
        if not info:
            return self._err(f"'{company_name}' ê³µì‹œì‹¤ ë¯¸ë“±ë¡")
        if not self._launch():
            return self._err("ì‹¤ì‹œê°„ ê³µì‹œì‹¤ í¬ë¡¤ë§ì€ ì„œë²„ í™˜ê²½ì—ì„œ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. DB ìºì‹œ ê²€ìƒ‰ì„ ì´ìš©í•˜ì„¸ìš”.")

        res = dict(pdf_url="", period="", revision_date="",
                   confidence=0, reason="", candidates_count=0, error="")
        try:
            page = self._browser.new_page()
            page.set_extra_http_headers(self._STEALTH_HEADERS)
            # navigator.webdriver ì†ì„± ì œê±° (ë´‡ íƒì§€ ìš°íšŒ)
            page.add_init_script(
                "Object.defineProperty(navigator,'webdriver',{get:()=>undefined})"
            )

            search_url = f"{info['url']}?{info['p']}={requests.utils.quote(product_name)}"
            if not self._safe_goto(page, search_url):
                self._safe_goto(page, info["url"])
                try:
                    page.fill(f"input[name='{info['p']}']", product_name)
                    page.keyboard.press("Enter")
                    time.sleep(self._NAV_WAIT)
                except Exception:
                    pass

            candidates = self._extract_candidates(page, product_name)

            # (3) íŒë§¤ì¤‘ì§€ íƒ­ ìë™ íƒìƒ‰: ê²°ê³¼ ì—†ìœ¼ë©´ íŒë§¤ì¤‘ì§€ íƒ­ í´ë¦­ ì‹œë„
            if not candidates:
                candidates = self._try_discontinued_tab(page, product_name)

            res["candidates_count"] = len(candidates)

            if not candidates:
                res["error"]  = "ê³µì‹œì‹¤ì—ì„œ PDF ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                res["reason"] = "ìƒí’ˆëª… ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ë˜ëŠ” JS ë Œë”ë§ í•„ìš”"
                return res

            best = DateRangeMatcher.best_match(join_date, candidates)
            if best:
                res.update(
                    pdf_url       = self._resolve_url(info["base"], best["url"]),
                    period        = best["period"],
                    revision_date = best["revision_date"],
                    confidence    = 92,
                    reason        = (
                        f"íŒë§¤ ê¸°ê°„ '{best['period']}'ì´ ê°€ì…ì¼ì {join_date}ë¥¼ í¬í•¨. "
                        f"ê°œì •ì¼: {best['revision_date'] or 'ë¯¸í™•ì¸'}. "
                        f"ì´ {len(candidates)}ê°œ í›„ë³´ ì¤‘ ìµœì  ì„ íƒ."
                    ),
                )
            else:
                fb = candidates[0]
                res.update(
                    pdf_url    = self._resolve_url(info["base"], fb["url"]),
                    period     = fb["period"],
                    confidence = 40,
                    reason     = (
                        f"ê°€ì…ì¼ì {join_date} ë§¤ì¹­ ì‹¤íŒ¨. "
                        f"ì²« ë²ˆì§¸ ê²°ê³¼({fb['text'][:60]}) ë°˜í™˜. ìˆ˜ë™ í™•ì¸ ê¶Œì¥."
                    ),
                )
        except Exception as e:
            res["error"] = str(e)[:300]
        finally:
            self._close()
        return res

    @staticmethod
    def _err(msg: str) -> dict:
        return dict(pdf_url="", period="", revision_date="",
                    confidence=0, reason=msg, candidates_count=0, error=msg)


# ---------------------------------------------------------------------------
# 4. JIT ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸
# ---------------------------------------------------------------------------
class JITPipelineRunner:
    """
    PDF URL â†’ pdfplumber ì²­í‚¹ â†’ Supabase gk_policy_terms í…Œì´ë¸” ì ì¬.

    í•„ìš” DDL (Supabase SQL Editorì—ì„œ 1íšŒ ì‹¤í–‰):
        CREATE TABLE IF NOT EXISTS gk_policy_terms (
            id           BIGSERIAL PRIMARY KEY,
            company      TEXT NOT NULL,
            product      TEXT NOT NULL,
            join_date    TEXT,
            pdf_url      TEXT,
            chunk_idx    INT  DEFAULT 0,
            chunk_text   TEXT NOT NULL,
            char_count   INT  DEFAULT 0,
            content_hash TEXT,
            indexed_at   TIMESTAMPTZ DEFAULT now()
        );
        CREATE UNIQUE INDEX IF NOT EXISTS idx_gk_policy_terms_hash
            ON gk_policy_terms(content_hash);
    """

    TABLE         = "gk_policy_terms"
    CHUNK_SIZE    = 800
    CHUNK_OVERLAP = 100

    def __init__(self, sb_client):
        self.sb = sb_client

    def _download_pdf(self, url: str) -> Optional[bytes]:
        try:
            resp = requests.get(url, headers={
                "User-Agent": "Mozilla/5.0 Chrome/120.0.0.0",
                "Accept": "application/pdf,*/*",
            }, timeout=30)
            resp.raise_for_status()
            return resp.content
        except Exception:
            return None

    def _pdf_to_chunks(self, pdf_bytes: bytes) -> list:
        full_text = ""
        # 1ì°¨: pdfplumber (í…ìŠ¤íŠ¸ PDF)
        try:
            import pdfplumber
            with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                full_text = "\n".join(p.extract_text() or "" for p in pdf.pages)
        except Exception:
            pass
        # 2ì°¨: pypdf fallback (ì•”í˜¸í™”/êµ¬í˜• PDF ëŒ€ì‘)
        if not full_text.strip():
            try:
                from pypdf import PdfReader
                reader = PdfReader(io.BytesIO(pdf_bytes))
                full_text = "\n".join(
                    (page.extract_text() or "") for page in reader.pages
                )
            except Exception:
                pass
        # 3ì°¨: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ì „ ì‹¤íŒ¨ ì‹œ ê²½ê³  ë¡œê·¸
        if not full_text.strip():
            logger.warning("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ â€” ì´ë¯¸ì§€ ì „ìš© PDFì´ê±°ë‚˜ ì•”í˜¸í™”ë¨")
            return []
        step = self.CHUNK_SIZE - self.CHUNK_OVERLAP
        return [
            full_text[i: i + self.CHUNK_SIZE].strip()
            for i in range(0, len(full_text), step)
            if len(full_text[i: i + self.CHUNK_SIZE].strip()) > 50
        ]

    def _upsert(self, company, product, join_date, pdf_url, idx, text,
                revision_date: str = "", period: str = "") -> bool:
        h = hashlib.sha256(text.encode("utf-8", errors="replace")).hexdigest()
        if not self.sb:
            return False
        try:
            self.sb.table(self.TABLE).upsert(
                {"company": company, "product": product, "join_date": join_date,
                 "pdf_url": pdf_url, "chunk_idx": idx,
                 "chunk_text": text[:4000], "char_count": len(text),
                 "content_hash": h,
                 "revision_date": revision_date,   # (4) ë©”íƒ€ë°ì´í„° ì•„ì¹´ì´ë¸Œ
                 "sale_period": period,
                 "indexed_at": datetime.utcnow().isoformat()},
                on_conflict="content_hash",
            ).execute()
            return True
        except Exception:
            return False

    def is_cached(self, company: str, product: str, join_date: str) -> bool:
        if not self.sb:
            return False
        try:
            r = (self.sb.table(self.TABLE)
                 .select("id", count="exact")
                 .eq("company", company).eq("product", product).eq("join_date", join_date)
                 .limit(1).execute())
            return (r.count or 0) > 0
        except Exception:
            return False

    def run(self, company: str, product: str, join_date: str,
            pdf_url: str, progress_cb=None) -> dict:
        def _log(msg):
            if progress_cb:
                progress_cb(msg)

        res = dict(ok=False, chunks_indexed=0, chunks_failed=0,
                   pdf_bytes_size=0, error="")

        _log(f"ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {pdf_url[:80]}...")
        pdf_bytes = self._download_pdf(pdf_url)
        if not pdf_bytes:
            res["error"] = "PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. URL ì ‘ê·¼ ë¶ˆê°€ ë˜ëŠ” ë³´ì•ˆ ì°¨ë‹¨."
            _log(f"âŒ {res['error']}")
            return res

        res["pdf_bytes_size"] = len(pdf_bytes)
        _log(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(pdf_bytes)//1024}KB). í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")

        chunks = self._pdf_to_chunks(pdf_bytes)
        if not chunks:
            res["error"] = "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì´ë¯¸ì§€ PDF ë˜ëŠ” ì•”í˜¸í™”)."
            _log(f"âŒ {res['error']}")
            return res

        _log(f"ğŸ“„ {len(chunks)}ê°œ ì²­í¬ â†’ Supabase ì ì¬ ì¤‘...")
        for idx, chunk in enumerate(chunks):
            if self._upsert(company, product, join_date, pdf_url, idx, chunk):
                res["chunks_indexed"] += 1
            else:
                res["chunks_failed"] += 1
            if progress_cb and idx % 10 == 0:
                progress_cb(f"  ì²­í¬ {idx+1}/{len(chunks)} ì ì¬...")

        res["ok"] = res["chunks_indexed"] > 0
        _log(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {res['chunks_indexed']}ê°œ ì„±ê³µ / {res['chunks_failed']}ê°œ ì‹¤íŒ¨")

        # (5) ì˜¤ë¥˜ ì•Œë¦¼: ì¸ë±ì‹± ì „ì²´ ì‹¤íŒ¨ ì‹œ Supabase ì˜¤ë¥˜ ë¡œê·¸ ê¸°ë¡
        if not res["ok"]:
            _write_crawl_error_log(self.sb, company, product, join_date,
                                   pdf_url, "ì¸ë±ì‹± ì „ì²´ ì‹¤íŒ¨")
        return res

    def search_terms(self, company: str, product: str, keyword: str, limit: int = 5) -> list:
        """ì¸ë±ì‹±ëœ ì•½ê´€ì—ì„œ í‚¤ì›Œë“œ ILIKE ê²€ìƒ‰"""
        if not self.sb:
            return []
        try:
            r = (self.sb.table(self.TABLE)
                 .select("chunk_text, chunk_idx, join_date, pdf_url")
                 .eq("company", company).eq("product", product)
                 .ilike("chunk_text", f"%{keyword}%")
                 .order("chunk_idx").limit(limit).execute())
            return r.data or []
        except Exception:
            return []


# ---------------------------------------------------------------------------
# 4-b. ì˜¤ë¥˜ ì•Œë¦¼ ì‹œìŠ¤í…œ (5ë²ˆ ê¸°ëŠ¥)
# ---------------------------------------------------------------------------
_ERROR_LOG_TABLE = "gk_crawl_error_log"
"""
DDL (Supabase SQL Editorì—ì„œ 1íšŒ ì‹¤í–‰):
    CREATE TABLE IF NOT EXISTS gk_crawl_error_log (
        id          BIGSERIAL PRIMARY KEY,
        company     TEXT,
        product     TEXT,
        join_date   TEXT,
        pdf_url     TEXT,
        error_msg   TEXT,
        logged_at   TIMESTAMPTZ DEFAULT now()
    );
"""

def _write_crawl_error_log(
    sb_client, company: str, product: str, join_date: str,
    pdf_url: str, error_msg: str
):
    """í¬ë¡¤ë§/ì¸ë±ì‹± ì‹¤íŒ¨ ì‹œ Supabase ì˜¤ë¥˜ ë¡œê·¸ í…Œì´ë¸”ì— ê¸°ë¡."""
    if not sb_client:
        logger.error(f"[CrawlError] {company}/{product}/{join_date}: {error_msg}")
        return
    try:
        sb_client.table(_ERROR_LOG_TABLE).insert({
            "company": company, "product": product,
            "join_date": join_date, "pdf_url": pdf_url,
            "error_msg": error_msg[:500],
            "logged_at": datetime.utcnow().isoformat(),
        }).execute()
    except Exception as e:
        logger.error(f"[CrawlError ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨] {e}")


def get_crawl_error_logs(sb_client, limit: int = 50) -> list:
    """ê´€ë¦¬ììš©: ìµœê·¼ í¬ë¡¤ë§ ì˜¤ë¥˜ ë¡œê·¸ ì¡°íšŒ."""
    if not sb_client:
        return []
    try:
        r = (sb_client.table(_ERROR_LOG_TABLE)
             .select("*")
             .order("logged_at", desc=True)
             .limit(limit)
             .execute())
        return r.data or []
    except Exception:
        return []


# ---------------------------------------------------------------------------
# 5. í†µí•© JIT ì¡°íšŒ ì§„ì…ì  (app.pyì—ì„œ ë‹¨ì¼ í˜¸ì¶œ)
# ---------------------------------------------------------------------------
def run_jit_policy_lookup(
    company_name: str,
    product_name: str,
    join_date: str,
    sb_client,
    progress_cb=None,
) -> dict:
    """
    JIT ì•½ê´€ ì¡°íšŒ ì „ì²´ íŒŒì´í”„ë¼ì¸ ë‹¨ì¼ ì§„ì…ì .

    íë¦„:
      1. Supabase DB ìºì‹œ í™•ì¸ (ì´ë¯¸ ì¸ë±ì‹± â†’ ì¦‰ì‹œ ë°˜í™˜)
      2. ì—†ìœ¼ë©´ ê³µì‹œì‹¤ í¬ë¡¤ë§ â†’ PDF URL íšë“
      3. JIT ì¸ë±ì‹± â†’ Supabase ì ì¬

    ë°˜í™˜:
      {
        "cached": bool,
        "pdf_url": str,
        "period": str,
        "confidence": int,       # 0~100
        "reason": str,
        "chunks_indexed": int,
        "error": str,
      }
    """

    def _log(msg: str):
        if progress_cb:
            progress_cb(msg)

    pipeline = JITPipelineRunner(sb_client)
    result   = dict(cached=False, pdf_url="", period="",
                    confidence=0, reason="", chunks_indexed=0, error="")

    # â”€â”€ Step 1: ìºì‹œ í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if pipeline.is_cached(company_name, product_name, join_date):
        result.update(cached=True, confidence=100,
                      reason="Supabase DBì— ì´ë¯¸ ì¸ë±ì‹±ëœ ì•½ê´€ì…ë‹ˆë‹¤. ì¦‰ì‹œ ê²€ìƒ‰ ê°€ëŠ¥.")
        _log("âœ… DB ìºì‹œ íˆíŠ¸ â€” ê³µì‹œì‹¤ í¬ë¡¤ë§ ìƒëµ")
        return result

    _log(f"ğŸ” DB ë¯¸ë“±ë¡ â†’ ê³µì‹œì‹¤ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹œì‘ "
         f"({company_name} / {product_name} / ê°€ì…ì¼ {join_date})")

    # â”€â”€ Step 2: ê³µì‹œì‹¤ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    crawl = PolicyDisclosureCrawler().fetch(company_name, product_name, join_date)
    result.update(pdf_url=crawl["pdf_url"], period=crawl["period"],
                  confidence=crawl["confidence"], reason=crawl["reason"],
                  error=crawl["error"])

    if not crawl["pdf_url"]:
        _log(f"âŒ ê³µì‹œì‹¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl['error']}")
        # (5) ì˜¤ë¥˜ ì•Œë¦¼: í¬ë¡¤ë§ ì‹¤íŒ¨ ë¡œê·¸ ê¸°ë¡
        _write_crawl_error_log(
            sb_client, company_name, product_name, join_date,
            "", crawl["error"]
        )
        return result

    _log(f"âœ… ì•½ê´€ PDF í™•ë³´ (ì‹ ë¢°ë„ {crawl['confidence']}%) â†’ ì¸ë±ì‹± ì‹œì‘")

    # â”€â”€ Step 3: JIT ì¸ë±ì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pipe_res = pipeline.run(
        company_name, product_name, join_date, crawl["pdf_url"], progress_cb=_log
    )
    result["chunks_indexed"] = pipe_res["chunks_indexed"]
    if pipe_res["error"]:
        result["error"] = pipe_res["error"]

    return result


# ---------------------------------------------------------------------------
# 6. í•©ì„± ë°ì´í„° ìƒì„± (Synthetic QA Generator) â€” Gemini ê¸°ë°˜
# ---------------------------------------------------------------------------

# í•µì‹¬ ì¡°í•­ ì„¹ì…˜ í‚¤ì›Œë“œ (SDG ì§‘ì¤‘ ëŒ€ìƒ)
_CORE_SECTION_KEYWORDS = [
    "ë³´ìƒí•˜ëŠ” ì†í•´", "ë³´ìƒí•˜ì§€ ì•ŠëŠ” ì†í•´", "ë©´ì±…", "ë©´ì±…ì‚¬í•­", "ë©´ì±…ì¡°í•­",
    "ì§€ê¸‰ ì‚¬ìœ ", "ì§€ê¸‰ì‚¬ìœ ", "ë³´í—˜ê¸ˆ ì§€ê¸‰", "ì§€ê¸‰ ê¸°ì¤€", "ì§€ê¸‰ê¸°ì¤€",
    "ë³´ì¥ë‚´ìš©", "ë³´ì¥ ë‚´ìš©", "ë‹´ë³´", "íŠ¹ì•½", "ì£¼ìš” ë³´ì¥",
    "ì§„ë‹¨", "ìˆ˜ìˆ ", "ì…ì›", "í†µì›", "ì¬í™œ",
    "ê³ ì§€ì˜ë¬´", "ê³„ì•½ ì „ ì•Œë¦´ ì˜ë¬´", "ì•Œë¦´ ì˜ë¬´",
    "ë³´í—˜ë£Œ ë‚©ì…", "ë‚©ì…ë©´ì œ", "ì‹¤íš¨", "í•´ì§€", "í™˜ê¸‰",
]


class SyntheticQAGenerator:
    """
    ë³´í—˜ ì•½ê´€ í…ìŠ¤íŠ¸ â†’ Gemini Flash(ì €ë ´)ë¡œ í•µì‹¬ ì¡°í•­ ì„ ë³„
    â†’ Gemini Pro(ê³ ì„±ëŠ¥)ë¡œ í•©ì„± QA 20ê°œ ìƒì„±
    â†’ Supabase gk_policy_terms_qa í…Œì´ë¸”ì— ì›ë¬¸+QA ë³‘ë ¬ ì ì¬.

    DDL (Supabase SQL Editorì—ì„œ 1íšŒ ì‹¤í–‰):
        CREATE TABLE IF NOT EXISTS gk_policy_terms_qa (
            id            BIGSERIAL PRIMARY KEY,
            company       TEXT NOT NULL,
            product       TEXT NOT NULL,
            join_date     TEXT,
            section_type  TEXT,        -- 'original' | 'synthetic_q' | 'synthetic_qa'
            chunk_idx     INT  DEFAULT 0,
            chunk_text    TEXT NOT NULL,
            char_count    INT  DEFAULT 0,
            content_hash  TEXT,
            indexed_at    TIMESTAMPTZ DEFAULT now()
        );
        CREATE UNIQUE INDEX IF NOT EXISTS idx_gk_policy_terms_qa_hash
            ON gk_policy_terms_qa(content_hash);
    """

    TABLE_QA    = "gk_policy_terms_qa"
    CHUNK_SIZE  = 600
    MAX_CHUNKS_FOR_SDG = 30   # SDG ëŒ€ìƒ ì²­í¬ ìµœëŒ€ ìˆ˜ (ë¹„ìš© ì œì–´)

    # í•µì‹¬ ì¡°í•­ ì„¹ì…˜ ê°ì§€ (CORE_SECTION_KEYWORDS)
    _CORE_KW = _CORE_SECTION_KEYWORDS

    # Gemini ëª¨ë¸ ì„¤ì •
    # - ì§ˆë¬¸ ìƒì„±(ê³ ì„±ëŠ¥): gemini-2.0-flash  â† GPT-4o ëŒ€ì‹  Gemini ê³„ì—´ ìµœê³ ì„±ëŠ¥
    # - ì €ì¥/ê²€ìƒ‰ìš© ì„ë² ë”©: í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì €ì¥ (sentence-transformersëŠ” JITPipelineì´ ë‹´ë‹¹)
    MODEL_SDG   = "gemini-2.0-flash"       # í•©ì„± ì§ˆë¬¸ ìƒì„± (ê³ í’ˆì§ˆ)
    MODEL_CHEAP = "gemini-2.0-flash-lite"  # í•µì‹¬ ì¡°í•­ ì„ ë³„ (ì €ë¹„ìš©)

    def __init__(self, sb_client, gemini_client=None):
        self.sb     = sb_client
        self._gc    = gemini_client   # google.genai client (ì—†ìœ¼ë©´ ìƒëµ)

    # â”€â”€ í•µì‹¬ ì¡°í•­ ê°ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _is_core_section(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì— í•µì‹¬ ì¡°í•­ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True"""
        return any(kw in text for kw in self._CORE_KW)

    # â”€â”€ í•©ì„± QA ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _generate_qa(self, chunk_text: str, company: str, product: str) -> list:
        """
        Geminië¡œ ì•½ê´€ ì²­í¬ â†’ ì˜ˆìƒ ì§ˆë¬¸ 20ê°œ ìƒì„±.
        ë°˜í™˜: ["ì§ˆë¬¸1", "ì§ˆë¬¸2", ...]
        """
        if not self._gc:
            return []

        prompt = (
            f"ë„ˆëŠ” ë² í…Œë‘ ë³´í—˜ ì„¤ê³„ì‚¬ì•¼. ë‹¤ìŒ ë³´í—˜ ì•½ê´€ ë‚´ìš©ì„ ë³´ê³ , "
            f"ê³ ê°ë“¤ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ì§ˆë¬¸ 20ê°œë¥¼ ë§Œë“¤ì–´ì¤˜.\n\n"
            f"[ë³´í—˜ì‚¬] {company}\n"
            f"[ìƒí’ˆëª…] {product}\n\n"
            f"[ì•½ê´€ ë‚´ìš©]\n{chunk_text[:1200]}\n\n"
            f"[ìš”êµ¬ì‚¬í•­]\n"
            f"- ì§ˆë¬¸ì€ \"ì•” ì§„ë‹¨ ì‹œ ì–¼ë§ˆë¥¼ ë°›ë‚˜ìš”?\"ì™€ ê°™ì´ êµ¬ì–´ì²´ë¡œ ì‘ì„±í•  ê²ƒ.\n"
            f"- ë³´í—˜ê¸ˆ ì§€ê¸‰ ì¡°ê±´, ë©´ì±… ì¡°í•­, íŠ¹ì•½ ì‚¬í•­ì— ì§‘ì¤‘í•  ê²ƒ.\n"
            f"- ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œ ì¤„ì— í•˜ë‚˜ì”© ì§ˆë¬¸ë§Œ ë‚˜ì—´í•  ê²ƒ. ë²ˆí˜¸Â·ê¸°í˜¸ ì—†ì´ ì§ˆë¬¸ í…ìŠ¤íŠ¸ë§Œ.\n"
            f"- í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•  ê²ƒ."
        )
        try:
            from google.genai import types as _gt
            resp = self._gc.models.generate_content(
                model=self.MODEL_SDG,
                contents=prompt,
                config=_gt.GenerateContentConfig(
                    temperature=0.7,
                    max_output_tokens=1024,
                ),
            )
            raw = (resp.text or "").strip()
            questions = [
                ln.strip().lstrip("0123456789.-) ").strip()
                for ln in raw.split("\n")
                if ln.strip() and len(ln.strip()) > 5
            ]
            return questions[:20]
        except Exception:
            return []

    # â”€â”€ Supabase upsert â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _upsert_qa(self, company: str, product: str, join_date: str,
                   section_type: str, idx: int, text: str) -> bool:
        h = hashlib.sha256(text.encode("utf-8", errors="replace")).hexdigest()
        if not self.sb:
            return False
        try:
            self.sb.table(self.TABLE_QA).upsert(
                {"company": company, "product": product, "join_date": join_date,
                 "section_type": section_type, "chunk_idx": idx,
                 "chunk_text": text[:4000], "char_count": len(text),
                 "content_hash": h},
                on_conflict="content_hash",
            ).execute()
            return True
        except Exception:
            return False

    # â”€â”€ ë©”ì¸ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run(self, company: str, product: str, join_date: str,
            chunks: list, progress_cb=None) -> dict:
        """
        chunks: JITPipelineRunner._pdf_to_chunks() ê²°ê³¼ (str ë¦¬ìŠ¤íŠ¸)
        íë¦„:
          1. ì›ë¬¸ ì²­í¬ â†’ gk_policy_terms_qa ì €ì¥ (section_type='original')
          2. í•µì‹¬ ì¡°í•­ ì„ ë³„ (CORE_SECTION_KEYWORDS í¬í•¨ ì²­í¬ë§Œ)
          3. ì„ ë³„ëœ ì²­í¬ â†’ Gemini SDG â†’ í•©ì„± QA ìƒì„±
          4. "ì§ˆë¬¸\në‹µë³€ê·¼ê±°: ì›ë¬¸" í˜•íƒœë¡œ gk_policy_terms_qa ì €ì¥ (section_type='synthetic_qa')
        """
        def _log(msg: str):
            if progress_cb:
                progress_cb(msg)

        res = dict(original_saved=0, core_chunks=0,
                   qa_generated=0, qa_saved=0, error="")

        # Step 1: ì›ë¬¸ ì €ì¥
        _log("ğŸ“„ ì›ë¬¸ ì²­í¬ ì €ì¥ ì¤‘...")
        for idx, chunk in enumerate(chunks):
            if self._upsert_qa(company, product, join_date, "original", idx, chunk):
                res["original_saved"] += 1

        # Step 2: í•µì‹¬ ì¡°í•­ ì„ ë³„
        core_chunks = [c for c in chunks if self._is_core_section(c)]
        core_chunks  = core_chunks[:self.MAX_CHUNKS_FOR_SDG]
        res["core_chunks"] = len(core_chunks)
        _log(f"ğŸ¯ í•µì‹¬ ì¡°í•­ {len(core_chunks)}ê°œ ì²­í¬ ì„ ë³„ ì™„ë£Œ (SDG ëŒ€ìƒ)")

        if not core_chunks:
            _log("â„¹ï¸ í•µì‹¬ ì¡°í•­ í‚¤ì›Œë“œ ë¯¸í¬í•¨ â€” SDG ìƒëµ. ì›ë¬¸ë§Œ ì €ì¥ë¨.")
            return res

        if not self._gc:
            _log("âš ï¸ Gemini í´ë¼ì´ì–¸íŠ¸ ë¯¸ì—°ê²° â€” SDG ìƒëµ. ì›ë¬¸ë§Œ ì €ì¥ë¨.")
            return res

        # Step 3 & 4: SDG ì‹¤í–‰
        _log(f"ğŸ¤– Gemini({self.MODEL_SDG}) SDG ì‹œì‘ â€” í•µì‹¬ {len(core_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬...")
        qa_idx = len(chunks)   # ì›ë¬¸ ì´í›„ idx ë¶€í„° ì‹œì‘
        for c_idx, chunk in enumerate(core_chunks):
            _log(f"  [{c_idx+1}/{len(core_chunks)}] í•©ì„± ì§ˆë¬¸ ìƒì„± ì¤‘...")
            questions = self._generate_qa(chunk, company, product)
            res["qa_generated"] += len(questions)

            for q in questions:
                combined = f"ì§ˆë¬¸: {q}\në‹µë³€ê·¼ê±°: {chunk[:600]}"
                if self._upsert_qa(company, product, join_date,
                                   "synthetic_qa", qa_idx, combined):
                    res["qa_saved"] += 1
                    qa_idx += 1

        _log(
            f"âœ… SDG ì™„ë£Œ: ì›ë¬¸ {res['original_saved']}ê°œ + "
            f"í•©ì„± QA {res['qa_saved']}ê°œ ì €ì¥"
        )
        return res

    def search_semantic(self, company: str, product: str,
                        keyword: str, limit: int = 5,
                        include_synthetic: bool = True) -> list:
        """
        gk_policy_terms_qaì—ì„œ ILIKE ê²€ìƒ‰.
        include_synthetic=Trueë©´ ì›ë¬¸+í•©ì„±QA ëª¨ë‘, Falseë©´ ì›ë¬¸ë§Œ.
        """
        if not self.sb:
            return []
        try:
            q = (self.sb.table(self.TABLE_QA)
                 .select("chunk_text, chunk_idx, section_type, join_date")
                 .eq("company", company).eq("product", product)
                 .ilike("chunk_text", f"%{keyword}%")
                 .order("section_type").order("chunk_idx")
                 .limit(limit))
            if not include_synthetic:
                q = q.eq("section_type", "original")
            return q.execute().data or []
        except Exception:
            return []
